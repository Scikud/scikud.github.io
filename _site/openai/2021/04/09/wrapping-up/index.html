<!DOCTYPE html>
<html lang="en">
<head>
    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Wrapping Up &#8211; ForceMultiplied</title>
    <link rel="dns-prefetch" href="//fonts.googleapis.com">
    <link rel="dns-prefetch" href="//fonts.gstatic.com">
    <link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com">
    <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="All good things">
    <link rel="manifest" type="application/manifest+json; charset=utf-8" href="/manifest.json" />
    <meta name="robots" content="all">
    <meta name="author" content="Kudzo Ahegbebu">
    
    <meta name="keywords" content="OpenAI">
    <link rel="canonical" href="http://localhost:4000/openai/2021/04/09/wrapping-up/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for ForceMultiplied" href="/feed.xml" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?202104141535" type="text/css">

    <!-- Fonts -->
    
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
    

    <!-- MathJax -->
    

    <!-- Verifications -->
    
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Wrapping Up">
    <meta property="og:description" content=".">
    <meta property="og:url" content="http://localhost:4000/openai/2021/04/09/wrapping-up/">
    <meta property="og:site_name" content="ForceMultiplied">
    

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    
    <meta name="twitter:title" content="Wrapping Up" />
    <meta name="twitter:description" content="All good things" />
    <meta name="twitter:url" content="http://localhost:4000/openai/2021/04/09/wrapping-up/" />
    

    <!-- Icons -->
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/favicon-160x160.png" sizes="160x160">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
    <link rel="shortcut icon" href="/favicon.ico">

    
    <script type="text/javascript">
       (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
       (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
       m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
       })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
       ga('create', 'UA-110849882-1', 'auto');
       ga('send', 'pageview');
    </script>
    
</head>

<body class="site animated fade-in-down">
  
	

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="/" class="site-title">ForceMultiplied</a>
      <nav class="site-nav">
        



    
    
    
    
        <a class="nav-link" href="/404.html">404</a>
    

    

    
    
    
    
        <a class="nav-link" href="/contact/">Say Hello</a>
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    


      </nav>
      <div class="clearfix"></div>
      
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"></script>



<div class="post-header mb2">
  <h1>Wrapping Up</h1>
  <span class="post-meta">Apr 9, 2021</span><br>
  
  <span class="post-meta small">
    
    12 minute read
    
  </span>
</div>

<article class="post-content">
  <p><strong>TL;DR</strong> <em>A recap of my work as part of OpenAI‚Äôs scholar‚Äôs program. I introduce the ‚Äòtest time compute dream‚Äô and recap some of the early ways in which I attempted to explore this problem. I detail how attempts to realize the test time compute dream in the form of baking recurrence back into language models were largely unsuccesful. As a means to find signs of life, I transition to graph neural networks operating over the game of sudoku and find definitive signs of life. I also find that using the machinery from deep equilibrium models to refine the hidden state of a graph neural network works quite well for improving training speed and reducing memory footprint. Additionally, I find that instead of explicitly hand-coding the adjacencies for a graph neural network we can instead use an attention head from a transformer to learn the adjacencies from the raw unstructured data and then train this model end to end via a surrogate loss and reinforcement learning.</em></p>

<h2 id="at-long-last">At long last</h2>
<p>Wow, where to begin? The last couple of weeks have been a whirlwind but at long last i‚Äôve arrived at the end of the scholar‚Äôs program.</p>

<p>This blog post will largely be a written version of my presentation with about as much detail.
To recap, for all two of you who actually read this blog  (hi, mom üëã! ),my scholar‚Äôs project has been spent thinking about what i‚Äôll call the test time compute dream. Briefly: 
<em>Can we construct models that continuously improve their outputs the more compute we pout into them at test time?</em></p>

<h2 id="recurrence-alone-is-inadequate">Recurrence Alone is Inadequate</h2>
<p>Broadly I tend to mentally partition test time compute ideas from literature into two general categories..</p>

<ol>
  <li>
    <p><strong>Generalization Improvement mechanisms:</strong> These deal with the question of whether we can create models that leverage test time compute to learn more general algorithms instead of simple statistical association. Ideally, we‚Äôd like for to construct  algorithms that use the extra compute to resolve ambiguity or to refine their outputs using computation they‚Äôve already done or outputs they‚Äôve already produced as new inputs for future time steps. Ideally we would like to have some guarantees that these models are truly computational complete. The Universal Transformer is my canonical example of work in this category.</p>
  </li>
  <li>
    <p><strong>Efficiency Mechanisms:</strong> This largely deals with the question of whether we can decouple the amount of time it takes to run a model at inference from the number of parameters a model has. The motivation here is simple; we would like to  increase the representational capacity  of our model by increasing the total number of trainable parameters without simultaneously incurring increased computational penalty for those extra parameters.  Examples of literature of this variety includes eternal memory mechanisms (fast product keys etc)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup></p>
  </li>
</ol>

<p>By in large, this project largely focused on the former mechanism.</p>

<p>Particularly, for the first two thirds of this project I was interested in exploring the generalization properties of test time compute methods in the context of the <a href="https://github.com/Scikud/cityDataset">‚Äúshortest path dataset‚Äù</a> . I‚Äôve talked about the shortest path set <a href="https://scikud.github.io/openai/2021/01/15/road-so-far/">several times on this blog</a>, so I wont go into too much detail here. The important question being explored was whether, if we control for total training FLOP budget, does a model trained to leverage test time compute (in the form of top layer recurrence) ever approach the performance of a model that doesn‚Äôt use this recurrence but perhaps is larger or was trained for more total gradient steps overall.</p>

<p>The way I did this was by keeping the training budget fixed and then training recurrent models with a fixed number of time steps during training with loss evaluated at every time step and then evaluating those same models with more steps of recurrence at test time. What I was interested in here was whether we ever observe a phase transition whether the extra compute allows these models to catch up with larger models trained without this recurrence.</p>

<p>Obviously theres much nuance here ‚Äì the very nature this experiment has questionable foundations ‚Äì regardless, to the extent to which we can answer the  question above, the answer seems largely to be: <em>not really</em>. That is, irrespective  of the model type you attempt this scheme on (and I tried a wide array of models indeed) we never observe a phase transition where it becomes favorable to expend your training budget on training a model that uses this recurrence scheme over just training a non recurrent model for more gradient update steps.</p>

<p>Running a linear probe over the embedding space of these models reveals that they actually learn the locations of the cities fairly quickly (or at the very least something isometric to the locations). The trouble really does appear as though learning some general shortest path finding algorithm seems to be a sufficiently difficult task for all these state of the art models. Even if you argue that cross entropy loss or perplexity is a poor metric to measure performance on something like shortest path, their performance is actually even worse on more <a href="https://scikud.github.io/openai/2021/01/15/road-so-far/">sensible metrics</a>.</p>

<p>To be clear, we really don‚Äôt care <em>at all</em> about actually solving the shortest path task, it exists almost solely as a vessel to explore some of these test time compute generalization properties. The actual, absolute, performance on the dataset is largely irrelevant, what matters is the relative performance between models using test time recurrence and those models not leveraging it.</p>

<p>These negative findings hold true to domains outside of shortest path as well. Admittedly, the experiments here were much less extensive but this trend seems to be true even for tasks like sorting (even when models are trained with <em>more</em> than the \(n \log n\) recurrence steps we know are algorithmically sufficient for this domain).  It really does appear as though test time compute in the form of recurrence is insufficient to achieve the generalization properties we seek. More structure is required.</p>

<h3 id="graph-neural-networks-to-the-rescue">Graph Neural Networks to the rescue</h3>
<p>In search of this additional structure, I turned to graph neural networks.</p>
<p style="text-align: center;"><img src="/images/GNN_intro.png" alt="GNNIntroSlide" />.</p>

<p>Graph neural networks are neural networks that operate on graph structured data<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup>.</p>

<p>GNNs process this graph by iteratively performing a learned message passing operation between nodes in which the GNN attempts to refine it‚Äôs internal representation of the nodes. It does this by using a learned message passing mechanism where at each timestep the hidden state is updated in the following way:</p>

\[h_i = \phi(\, x_i, \, \bigoplus_{j \in N_i} \psi(x_i, x_j) \,)\]

<p>Above, \(h_i\) is the hidden state for some node i,  and \(x_i\) is the node embedding  for a particular node. Effectively, the update equation specifies that the hidden state at each  layer (or time step in our case) be updated by a function that takes in as inputs the node embedding, and all pairs of that nodes neighbors passed through some message passing function  \(\psi\) and then aggregated using some aggregation function.</p>

<p>The training regime was done in effectively the exact same way as the shortest model explorations where I force the model to make a prediction <em>at every timestep</em> and evaluate the loss at <em>every</em> timestep as well. This is done to ensure that the model is robust to being evaluated at <em>test</em> time with more evaluations than the model was evaluated with during training time.</p>

<h3 id="signs-of-life">Signs of Life</h3>

<p style="text-align: center;"><img src="/images/SudokuSolving.gif" alt="SudokuPlaying" />.</p>

<p>Above I embedded a movie of this GNN operating over some of the sudoku dataset. What‚Äôs particularly interesting is that it seems to prioritize using the extra test time compute (graph refinement steps) to  attend to tokens it had previously assigned high uncertainty (low probability) to in the previous time steps. In other words red things become green and green things tend to remain green. This is fascinating because it suggests that the probabilities we extract from the logits seem to have some semantically meaningful concept of uncertainty.</p>

<p>Of further interest is that fact that this GNN actually seems to do better the <em>more</em> iterations you give it. Particularly, if you evaluate it with more iterations than those it was trained with it continues to improve it‚Äôs accuracy in an almost monotonic way. Additionally, if you evaluate it on problems that are harder than the problems it was trained on, it actually still does reasonably well (check out the presentation for the actual graphs).</p>

<p>While nothing in the above is particularly <a href="https://arxiv.org/pdf/1711.08028.pdf">novel</a>, it does demonstrate that at least in principle the test time compute dream is possible. The key ingredient here seems to be related to the recurrence <em>plus</em> the message passing mechanism of these graph neural networks.</p>

<h3 id="can-we-do-better-deqs-to-the-rescue">Can we do better? DEQs to the rescue</h3>
<p>If the central argument here is <em>more test time compute in the form of iterations seems to be helpful</em> what if we could take this argument to the infinite limit. In order to do this we need to steal the machinery from deep equilibrium models. I‚Äôve written about deep equilibrium models several times before on this blog<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote">3</a></sup>, but the main take away here is that the graph refinement equation for graph neural networks is <em>exactly</em> a fixed point equation which means that the implicit function theorem allows us to use some arbitrary black box root finding algorithm to both evaluate the value of the function at the equilibrium point and the value of the gradients there as well.</p>

\[h_i = \phi(\, x_i, \, \bigoplus_{j \in N_i} \psi(x_i, x_j) \,)\]

\[g^t_i(h_i, x_i) = h^{t-1}_i - \phi(\, x^{t-1}_i, \, \bigoplus_{j \in N_i} \psi(x^{t-1}_i, x^{t-1}_j) \,)\]

<p>Trying this out, works surprisingly well, <em>at first</em>. The deep equilibrium GNN both trains much faster, seems to have better accuracy,  and has lower memory requirements than the traditional GNN.</p>

<p style="text-align: center;"><img src="/images/DEQGraphs.png" alt="SudokuPlaying" /></p>

<p>All is not well in paradise however; every single time I‚Äôve trained this model, i‚Äôve observed a strange crash in the training accuracy occur several hours in. The DEQ GNN will train with better accuracy than the traditional GNN, and then all of a sudden die wherein the accuracy plummets to zero. I‚Äôm still investigating this and while I have some suspicions about the cause (particularly instability caused by the growth of the spectral norm of the operators) it could also very well be a bug in my training loop. Either way, just mentioning these things here for the sake of completeness.</p>

<h2 id="learning-graph-adjacencies-using-policy-gradients">Learning Graph Adjacencies Using Policy Gradients</h2>

<p>All the above is well and good, however, there‚Äôs something a little strange about the way I‚Äôve seen graph neural networks used - the fact that the adjacencies must be encoded by hand. For the sudoku problem in particular, I had to explicitly bake in the fact that nodes that share a row, column or a cell are connected.  Could we instead learn the adjacencies from the raw unstructured data?</p>

<p>Here‚Äôs the idea: transformers are fairly adept at learning how relevant <em>pairs</em> of tokens are to each other. On the other hand GNNs seem to be good at performing well on relational reasoning style tasks particularly over graph structured data. What if we could use the attention head from a standard transformer to extract an adjacency matrix which we then feed into a GNN.</p>

<p>This scheme operates by first feeding our input into a small transformer which has a small modification in the top layer such that we use the probability scores to categorically sample the top K indicies which are most relevant (i.e for each input token what are corresponding <em>k</em> other tokens which the attention head assigns the highest normalized probability to). This then extracts K neighborhoods for each token which we than feed into our GNN.</p>

<p>Because sampling indices is a non differentiable operation, we need to compensate for this by using a gradient estimator. John Schulman has a great paper<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">4</a></sup> describing exactly how to do this in the general case for stochastic compute graphs.</p>

<p style="text-align: center;"><img src="/images/schulmanStochasticComputegraph.png" alt="Stochastic Compute Graph" />.</p>

<p>The formalism outlined in the paper  gives us a way to convert stochastic computation graphs into deterministic compute graphs and evaluate a ‚Äúsurrogate loss‚Äù that provides a mechanism to use standard backprop to arrive at an unbiased  gradient estimator through these stochastic nodes.</p>

<p>If you try all the above out it kind of works! Works in the sense that the model will achieve low training loss (as well as low validation loss). In reality however, there are several caveats here. The most obvious being the much slower training. Training this Frankenstein stochastic graph transformer took about 5 days compared to the 5 hours of training required for the traditional GNN. Additionally, performance is actually worse that the standard GNN ‚Äìpeaking at 62% best accuracy. Lastly, using vanilla policy gradients in this way has really high variance.</p>

<p style="text-align: center;"><img src="/images/validationLossStochasticGNNXformer.png" alt="Stochastic GNN Transformer Loss" /></p>

<p>These objections and caveats aside, the interesting thing here is that this demonstrates that <em>in principle</em> one <em>could</em> train a GNN with adjacencies learned from scratch as well. I can imagine this being useful in a number of ways, particularly in contexts where its useful to use the strong relational reasoning performance of graph neural networks but we‚Äôd like to learn the graphs dynamically or in a context dependent way.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>https://arxiv.org/abs/1907.05242¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>https://www.youtube.com/watch?v=uF53xsT7mjc Fantastic resource¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>https://scikud.github.io/openai/2020/12/20/troubles/¬†<a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>https://arxiv.org/pdf/1506.05254.pdf¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

</article>










      </div>
    </div>
  </div>

  <footer class="center">
  <div class="measure">
    <small>
      Theme available on <a href="https://github.com/johnotander/pixyll">GitHub</a>.
    </small>
  </div>
</footer>

<script type="text/javascript">
  if ("serviceWorker" in navigator) {
    navigator.serviceWorker.register("/sw.js")
  }
</script>
</body>
</html>
