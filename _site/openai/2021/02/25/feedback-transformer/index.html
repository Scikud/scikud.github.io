<!DOCTYPE html>
<html lang="en">
<head>
    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Crunchtime &#8211; ForceMultiplied</title>
    <link rel="dns-prefetch" href="//fonts.googleapis.com">
    <link rel="dns-prefetch" href="//fonts.gstatic.com">
    <link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com">
    <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Sharing Some Code">
    <link rel="manifest" type="application/manifest+json; charset=utf-8" href="/manifest.json" />
    <meta name="robots" content="all">
    <meta name="author" content="Kudzo Ahegbebu">
    
    <meta name="keywords" content="OpenAI">
    <link rel="canonical" href="http://localhost:4000/openai/2021/02/25/feedback-transformer/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for ForceMultiplied" href="/feed.xml" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?202102252333" type="text/css">

    <!-- Fonts -->
    
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
    

    <!-- MathJax -->
    

    <!-- Verifications -->
    
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Crunchtime">
    <meta property="og:description" content=".">
    <meta property="og:url" content="http://localhost:4000/openai/2021/02/25/feedback-transformer/">
    <meta property="og:site_name" content="ForceMultiplied">
    

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    
    <meta name="twitter:title" content="Crunchtime" />
    <meta name="twitter:description" content="Sharing Some Code" />
    <meta name="twitter:url" content="http://localhost:4000/openai/2021/02/25/feedback-transformer/" />
    

    <!-- Icons -->
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/favicon-160x160.png" sizes="160x160">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
    <link rel="shortcut icon" href="/favicon.ico">

    
    <script type="text/javascript">
       (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
       (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
       m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
       })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
       ga('create', 'UA-110849882-1', 'auto');
       ga('send', 'pageview');
    </script>
    
</head>

<body class="site animated fade-in-down">
  
	

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="/" class="site-title">ForceMultiplied</a>
      <nav class="site-nav">
        



    
    
    
    
        <a class="nav-link" href="/404.html">404</a>
    

    

    
    
    
    
        <a class="nav-link" href="/contact/">Say Hello</a>
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    

    
    
    
    

    


      </nav>
      <div class="clearfix"></div>
      
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"></script>



<div class="post-header mb2">
  <h1>Crunchtime</h1>
  <span class="post-meta">Feb 25, 2021</span><br>
  
  <span class="post-meta small">
    
    3 minute read
    
  </span>
</div>

<article class="post-content">
  <h3 id="utility-functions">Utility functions</h3>
<p>I’ve spent the past week writing some desperately neeeded utility functions to instrument the code that i’ve built out as part of the scholar’s program. Largely, i’ve been trying to gain a better understanding of exactly what’s going on inside these models as they operate over data continously. One lesson learned is on the importance of writing utility functions early, not only because it saves you valuable time when approaching deadlines, but also because being really explicit up front about the kind of data you would like to collect also informs what kind of experiments your’e likely to run.</p>

<h3 id="feedback-transformer">Feedback Transformer</h3>
<p>Last blog post, I mentioned I’ve been playing around with implementing the feedback transformer. The principle idea in the feedback transformer is to allow low level representation in transformers to attend to previous higher level representations. This modifies the computational path of the the traditional transformer architecture and transforms it something functionally resembelling an autoregressive RNN. I wrote out a quick and dirty implementation for this (below), which I’ll clean up and post on github at some point, along with the million other things I have to catch up on.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nx">MultiHeadedAttn</span><span class="p">(</span><span class="nx">nn</span><span class="p">.</span><span class="nx">Module</span><span class="p">):</span>
    <span class="nx">def</span> <span class="nx">__init__</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">Config</span><span class="p">):</span>
        <span class="k">super</span><span class="p">(</span><span class="nx">MultiHeadedAttn</span><span class="p">,</span> <span class="nb">self</span><span class="p">).</span><span class="nx">__init__</span><span class="p">()</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">c</span> <span class="o">=</span> <span class="nx">c</span> <span class="o">=</span> <span class="nx">Config</span>
        <span class="err">#</span> <span class="nx">Generates</span> <span class="nx">queries</span><span class="p">,</span> <span class="nx">keys</span><span class="p">,</span> <span class="nx">values</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">fc1</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">,</span> <span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">qkvSize</span><span class="p">)</span><span class="o">*</span><span class="nx">c</span><span class="p">.</span><span class="nx">numAttnHeads</span><span class="p">)</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">fc2</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">)</span>
        <span class="err">#</span> <span class="nx">Create</span> <span class="nx">Mask</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">register_buffer</span><span class="p">(</span>
            <span class="dl">"</span><span class="s2">causalMask</span><span class="dl">"</span><span class="p">,</span>
            <span class="nx">torch</span><span class="p">.</span><span class="nx">tril</span><span class="p">(</span><span class="nx">torch</span><span class="p">.</span><span class="nx">ones</span><span class="p">((</span><span class="nx">c</span><span class="p">.</span><span class="nx">blockSize</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">blockSize</span><span class="p">))))</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">register_buffer</span><span class="p">(</span>
            <span class="dl">"</span><span class="s2">padMask</span><span class="dl">"</span><span class="p">,</span>
            <span class="nx">torch</span><span class="p">.</span><span class="nx">ones</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">blockSize</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">blockSize</span><span class="p">))</span>

    <span class="nx">def</span> <span class="nx">forward</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">x</span><span class="p">,</span> <span class="nx">k</span><span class="p">,</span> <span class="nx">v</span><span class="p">):</span>
        <span class="nx">B</span><span class="p">,</span> <span class="nx">T</span><span class="p">,</span> <span class="nx">embdSize</span> <span class="o">=</span> <span class="nx">x</span><span class="p">.</span><span class="nx">shape</span>  <span class="err">#</span> <span class="nx">B</span> <span class="o">=</span><span class="nx">Batch</span> <span class="nx">size</span><span class="p">,</span>  <span class="nx">T</span> <span class="o">=</span> <span class="nx">numTokens</span>
        <span class="nx">h</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">c</span><span class="p">.</span><span class="nx">numAttnHeads</span>

        <span class="nx">q</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">fc1</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span>
        <span class="nx">q</span> <span class="o">=</span> <span class="nx">q</span><span class="p">.</span><span class="nx">reshape</span><span class="p">(</span><span class="nx">B</span><span class="p">,</span><span class="nx">h</span><span class="p">,</span><span class="nx">T</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="nx">k</span> <span class="o">=</span> <span class="nx">k</span><span class="p">.</span><span class="nx">reshape</span><span class="p">(</span><span class="nx">B</span><span class="p">,</span><span class="nx">h</span><span class="p">,</span><span class="nx">T</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="nx">v</span> <span class="o">=</span> <span class="nx">k</span><span class="p">.</span><span class="nx">reshape</span><span class="p">(</span><span class="nx">B</span><span class="p">,</span><span class="nx">h</span><span class="p">,</span><span class="nx">T</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="err">#</span> <span class="nx">God</span> <span class="nx">bless</span> <span class="nx">einsum</span>
        <span class="nx">attn</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">einsum</span><span class="p">(</span><span class="dl">'</span><span class="s1">bhij,bhkj-&gt;bhik</span><span class="dl">'</span><span class="p">,</span> <span class="nx">q</span><span class="p">,</span> <span class="nx">k</span><span class="p">)</span>
        <span class="nx">mask</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">unsqueeze</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nx">padMasks</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nx">repeat</span><span class="p">(</span>
            <span class="mi">1</span><span class="p">,</span> <span class="nx">h</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[...,</span> <span class="p">:</span><span class="nx">T</span><span class="p">,:</span><span class="nx">T</span><span class="p">]</span> <span class="o">*</span><span class="nb">self</span><span class="p">.</span><span class="nx">causalMask</span><span class="p">[:</span><span class="nx">T</span><span class="p">,</span> <span class="p">:</span><span class="nx">T</span><span class="p">]</span>
        <span class="nx">attn</span> <span class="o">=</span> <span class="nx">attn</span><span class="p">.</span><span class="nx">masked_fill</span><span class="p">(</span><span class="nx">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">.,</span> <span class="nx">float</span><span class="p">(</span><span class="dl">'</span><span class="s1">-inf</span><span class="dl">'</span><span class="p">))</span>
        <span class="nx">scores</span> <span class="o">=</span> <span class="nx">F</span><span class="p">.</span><span class="nx">softmax</span><span class="p">(</span><span class="nx">attn</span><span class="o">/</span><span class="nx">np</span><span class="p">.</span><span class="nx">sqrt</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nx">c</span><span class="p">.</span><span class="nx">qkvSize</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="nx">outpt</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">einsum</span><span class="p">(</span><span class="dl">'</span><span class="s1">bhij,bhjk-&gt;bhik</span><span class="dl">'</span><span class="p">,</span> <span class="nx">scores</span><span class="p">,</span> <span class="nx">v</span><span class="p">)</span>
        <span class="nx">outpt</span> <span class="o">=</span> <span class="nx">outpt</span><span class="p">.</span><span class="nx">view</span><span class="p">(</span><span class="nx">B</span><span class="p">,</span> <span class="nx">T</span><span class="p">,</span> <span class="nx">embdSize</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">self</span><span class="p">.</span><span class="nx">fc2</span><span class="p">(</span><span class="nx">outpt</span><span class="p">)</span>


<span class="kd">class</span> <span class="nx">DecoderBlock</span><span class="p">(</span><span class="nx">nn</span><span class="p">.</span><span class="nx">Module</span><span class="p">):</span>
    <span class="nx">def</span> <span class="nx">__init__</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">config</span><span class="p">):</span>
        <span class="k">super</span><span class="p">(</span><span class="nx">DecoderBlock</span><span class="p">,</span> <span class="nb">self</span><span class="p">).</span><span class="nx">__init__</span><span class="p">()</span>
        <span class="nx">c</span> <span class="o">=</span> <span class="nx">config</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">ln1</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">LayerNorm</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">)</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">attn</span> <span class="o">=</span> <span class="nx">MultiHeadedAttn</span><span class="p">(</span><span class="nx">config</span><span class="p">)</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">mlp</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Sequential</span><span class="p">(</span>
            <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="o">*</span><span class="mi">4</span><span class="p">),</span>
            <span class="nx">nn</span><span class="p">.</span><span class="nx">GELU</span><span class="p">(),</span>
            <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">ln2</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">LayerNorm</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">)</span>

    <span class="nx">def</span> <span class="nx">_setPadMasks</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">padMasks</span><span class="p">):</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">attn</span><span class="p">.</span><span class="nx">padMasks</span> <span class="o">=</span> <span class="nx">padMasks</span>
        
    <span class="nx">def</span> <span class="nx">forward</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">x</span><span class="p">,</span> <span class="nx">k</span><span class="p">,</span> <span class="nx">v</span><span class="p">):</span>
        <span class="nx">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">ln1</span><span class="p">(</span><span class="nx">x</span><span class="o">+</span><span class="nb">self</span><span class="p">.</span><span class="nx">attn</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="nx">k</span><span class="p">,</span><span class="nx">v</span><span class="p">))</span>
        <span class="nx">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">ln2</span><span class="p">(</span><span class="nx">x</span> <span class="o">+</span> <span class="nb">self</span><span class="p">.</span><span class="nx">mlp</span><span class="p">(</span><span class="nx">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="nx">x</span>


<span class="kd">class</span> <span class="nx">TinyFeedbackTransformer</span><span class="p">(</span><span class="nx">nn</span><span class="p">.</span><span class="nx">Module</span><span class="p">):</span>
    <span class="nx">def</span> <span class="nx">__init__</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">Config</span><span class="p">):</span>
        <span class="k">super</span><span class="p">(</span><span class="nx">TinyFeedbackTransformer</span><span class="p">,</span> <span class="nb">self</span><span class="p">).</span><span class="nx">__init__</span><span class="p">()</span>
        <span class="err">#</span> <span class="nx">Size</span> <span class="nx">assertions</span>
        <span class="nx">assert</span> <span class="nx">Config</span><span class="p">.</span><span class="nx">embdSize</span> <span class="o">&gt;=</span> <span class="nx">Config</span><span class="p">.</span><span class="nx">numAttnHeads</span>

        <span class="err">#</span> <span class="nx">Configuration</span> <span class="nx">stuff</span>
        <span class="nx">c</span> <span class="o">=</span> <span class="nx">Config</span>
        <span class="nx">c</span><span class="p">.</span><span class="nx">qkvSize</span> <span class="o">=</span> <span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span> <span class="c1">// c.numAttnHeads</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">config</span> <span class="o">=</span> <span class="nx">c</span>

        <span class="nb">self</span><span class="p">.</span><span class="nx">wordEmbedding</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Embedding</span><span class="p">(</span>
            <span class="nx">c</span><span class="p">.</span><span class="nx">paddingIndx</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">,</span> <span class="nx">padding_idx</span><span class="o">=</span><span class="nx">c</span><span class="p">.</span><span class="nx">paddingIndx</span><span class="p">)</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">posEmbedding</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Parameter</span><span class="p">(</span>
            <span class="nx">torch</span><span class="p">.</span><span class="nx">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">blockSize</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">))</span>

        <span class="err">#</span><span class="nx">New</span> <span class="nx">learnable</span> <span class="nx">parameters</span> <span class="k">for</span> <span class="nx">feedback</span> <span class="nx">transformer</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">memoryCoeff</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Parameter</span><span class="p">(</span><span class="nx">torch</span><span class="p">.</span><span class="nx">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">numLayers</span><span class="p">))</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">ffkv</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">,</span> <span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">qkvSize</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="nx">c</span><span class="p">.</span><span class="nx">numAttnHeads</span><span class="p">)</span>

        <span class="nb">self</span><span class="p">.</span><span class="nx">blocks</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="nx">DecoderBlock</span><span class="p">(</span><span class="nx">c</span><span class="p">)</span> <span class="k">for</span> <span class="nx">_</span> <span class="k">in</span> <span class="nx">range</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">numLayers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">ln1</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">LayerNorm</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">)</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">head</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">paddingIndx</span><span class="p">,</span> <span class="nx">bias</span><span class="o">=</span><span class="nx">False</span><span class="p">)</span>

        <span class="nb">self</span><span class="p">.</span><span class="nx">apply</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nx">_init_weights</span><span class="p">)</span>

    <span class="nx">def</span> <span class="nx">forward</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">indxs</span><span class="p">,</span> <span class="nx">padMasks</span><span class="p">):</span>
        <span class="k">for</span> <span class="nx">mod</span> <span class="k">in</span> <span class="nb">self</span><span class="p">.</span><span class="nx">blocks</span><span class="p">:</span>
            <span class="nx">mod</span><span class="p">.</span><span class="nx">_setPadMasks</span><span class="p">(</span><span class="nx">padMasks</span><span class="p">)</span>
        <span class="nx">numTokens</span> <span class="o">=</span> <span class="nx">indxs</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>


        <span class="err">#</span> <span class="nx">Combine</span> <span class="nx">word</span> <span class="nx">and</span> <span class="nx">position</span> <span class="nx">embeddings</span>
        <span class="nx">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">wordEmbedding</span><span class="p">(</span><span class="nx">indxs</span><span class="p">)</span>
        <span class="nx">pos</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">posEmbedding</span><span class="p">[:,</span> <span class="p">:</span><span class="nx">numTokens</span><span class="p">,</span> <span class="p">:]</span>
        <span class="nx">x</span> <span class="o">=</span> <span class="nx">x</span><span class="o">+</span><span class="nx">pos</span>

        <span class="err">#</span><span class="nx">Initalize</span> <span class="nx">the</span> <span class="nx">memory</span> <span class="nx">tensor</span>
        <span class="nx">memory</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">tensor</span><span class="p">([]).</span><span class="nx">to</span><span class="p">(</span><span class="nx">device</span><span class="p">)</span>
        <span class="nx">batchSize</span> <span class="o">=</span> <span class="nx">x</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>        
        <span class="nx">blockSize</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">config</span><span class="p">.</span><span class="nx">blockSize</span>
        <span class="nx">maxMemSize</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">config</span><span class="p">.</span><span class="nx">memorySize</span>
 

        <span class="nx">finalOutputs</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">tensor</span><span class="p">([]).</span><span class="nx">to</span><span class="p">(</span><span class="nx">device</span><span class="p">)</span>
        <span class="err">#</span><span class="nx">Pass</span> <span class="nx">through</span> <span class="nx">the</span> <span class="nx">transformer</span>
        <span class="k">for</span> <span class="nx">indx</span> <span class="k">in</span> <span class="nx">range</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="nx">currSlice</span> <span class="o">=</span> <span class="nx">x</span><span class="p">[:,</span> <span class="nx">indx</span><span class="p">,</span> <span class="p">...].</span><span class="nx">view</span><span class="p">(</span><span class="nx">batchSize</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="nx">inpt</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">cat</span><span class="p">((</span><span class="nx">memory</span><span class="p">,</span><span class="nx">currSlice</span><span class="p">),</span> <span class="nx">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="nx">inpt</span> <span class="o">=</span> <span class="nx">inpt</span><span class="p">[:,</span> <span class="o">-</span><span class="nx">maxMemSize</span><span class="p">:,</span> <span class="p">...]</span>

            <span class="err">#</span> <span class="nx">Grab</span> <span class="nx">the</span> <span class="nx">W_k</span> <span class="nx">and</span> <span class="nx">the</span> <span class="nx">W_v</span> <span class="nx">parameters</span>
            <span class="nx">wk</span><span class="p">,</span> <span class="nx">wv</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">ffkv</span><span class="p">(</span><span class="nx">inpt</span><span class="p">).</span><span class="nx">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

            <span class="nx">outputs</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">tensor</span><span class="p">([]).</span><span class="nx">to</span><span class="p">(</span><span class="nx">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="nx">decoderBlock</span> <span class="k">in</span> <span class="nb">self</span><span class="p">.</span><span class="nx">blocks</span><span class="p">:</span>
                <span class="nx">inpt</span> <span class="o">=</span> <span class="nx">decoderBlock</span><span class="p">(</span><span class="nx">inpt</span><span class="p">,</span> <span class="nx">wk</span><span class="p">,</span> <span class="nx">wv</span><span class="p">)</span>
                <span class="nx">outputs</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">cat</span><span class="p">((</span><span class="nx">outputs</span><span class="p">,</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">unsqueeze</span><span class="p">(</span><span class="nx">inpt</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:],</span><span class="mi">0</span><span class="p">)))</span>
            <span class="nx">currmemory</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">einsum</span><span class="p">(</span><span class="dl">'</span><span class="s1">il,lbd-&gt;bd</span><span class="dl">'</span><span class="p">,</span>
                                      <span class="nx">torch</span><span class="p">.</span><span class="nx">softmax</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nx">memoryCoeff</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="nx">outputs</span><span class="p">)</span>
            
            <span class="nx">memory</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">cat</span><span class="p">((</span><span class="nx">memory</span><span class="p">,</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">unsqueeze</span><span class="p">(</span><span class="nx">currmemory</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="nx">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="nx">memory</span> <span class="o">=</span> <span class="nx">memory</span><span class="p">[:,</span> <span class="o">-</span><span class="nx">maxMemSize</span><span class="p">:,</span> <span class="p">...]</span>
            <span class="nx">finalOutputs</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">cat</span><span class="p">((</span><span class="nx">finalOutputs</span><span class="p">,</span><span class="nx">torch</span><span class="p">.</span><span class="nx">unsqueeze</span><span class="p">(</span><span class="nx">inpt</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:],</span><span class="mi">1</span><span class="p">)),</span> <span class="nx">dim</span> <span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="nx">finalOutputs</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">head</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nx">ln1</span><span class="p">(</span><span class="nx">finalOutputs</span><span class="p">))</span>

        <span class="k">return</span> <span class="nx">finalOutputs</span>

    <span class="nx">def</span> <span class="nx">_init_weights</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">module</span><span class="p">):</span>
        <span class="k">if</span> <span class="nx">isinstance</span><span class="p">(</span><span class="nx">module</span><span class="p">,</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Embedding</span><span class="p">):</span>
            <span class="nx">d</span> <span class="o">=</span> <span class="p">(</span><span class="nx">module</span><span class="p">.</span><span class="nx">embedding_dim</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
            <span class="nx">module</span><span class="p">.</span><span class="nx">weight</span><span class="p">.</span><span class="nx">data</span><span class="p">.</span><span class="nx">normal_</span><span class="p">(</span><span class="nx">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nx">std</span><span class="o">=</span><span class="mf">0.125</span><span class="o">/</span><span class="nx">d</span><span class="p">)</span>
        <span class="k">if</span> <span class="nx">isinstance</span><span class="p">(</span><span class="nx">module</span><span class="p">,</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">):</span>
            <span class="nx">d</span> <span class="o">=</span> <span class="p">(</span><span class="nx">module</span><span class="p">.</span><span class="nx">in_features</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
            <span class="nx">module</span><span class="p">.</span><span class="nx">weight</span><span class="p">.</span><span class="nx">data</span><span class="p">.</span><span class="nx">normal_</span><span class="p">(</span><span class="nx">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nx">std</span><span class="o">=</span><span class="mf">0.125</span><span class="o">/</span><span class="nx">d</span><span class="p">)</span>
            <span class="k">if</span> <span class="nx">module</span><span class="p">.</span><span class="nx">bias</span> <span class="nx">is</span> <span class="nx">not</span> <span class="nx">None</span><span class="p">:</span>
                <span class="nx">module</span><span class="p">.</span><span class="nx">bias</span><span class="p">.</span><span class="nx">data</span><span class="p">.</span><span class="nx">zero_</span><span class="p">()</span>
        <span class="nx">elif</span> <span class="nx">isinstance</span><span class="p">(</span><span class="nx">module</span><span class="p">,</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">LayerNorm</span><span class="p">):</span>
            <span class="nx">module</span><span class="p">.</span><span class="nx">bias</span><span class="p">.</span><span class="nx">data</span><span class="p">.</span><span class="nx">zero_</span><span class="p">()</span>
            <span class="nx">module</span><span class="p">.</span><span class="nx">weight</span><span class="p">.</span><span class="nx">data</span><span class="p">.</span><span class="nx">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div></div>

<p>Anywho, that’s it for now. More to come in the coming weeks if I ever find time to organize my thoughts and my work. TTYL</p>

</article>










      </div>
    </div>
  </div>

  <footer class="center">
  <div class="measure">
    <small>
      Theme available on <a href="https://github.com/johnotander/pixyll">GitHub</a>.
    </small>
  </div>
</footer>

<script type="text/javascript">
  if ("serviceWorker" in navigator) {
    navigator.serviceWorker.register("/sw.js")
  }
</script>
</body>
</html>
