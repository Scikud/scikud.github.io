<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title></title>
    <link>http://localhost:4000</link>
    <description>
      .
    </description>
    
        
            <item>
                <title>Wrapping Up</title>
                <link>http://localhost:4000/openai/2021/04/09/wrapping-up/</link>
                <content:encoded>
                    <![CDATA[
                    <p><strong>TL;DR</strong> <em>A recap of my work as part of OpenAI‚Äôs scholar‚Äôs program. I introduce the ‚Äòtest time compute dream‚Äô and recap some of the early ways in which I attempted to explore this problem. I detail how attempts to realize the test time compute dream in the form of baking recurrence back into language models were largely unsuccesful. As a means to find signs of life, I transition to graph neural networks operating over the game of sudoku and find definitive signs of life. I also find that using the machinery from deep equilibrium models to refine the hidden state of a graph neural network works quite well for improving training speed and reducing memory footprint. Additionally, I find that instead of explicitly hand-coding the adjacencies for a graph neural network we can instead use an attention head from a transformer to learn the adjacencies from the raw unstructured data and then train this model end to end via a surrogate loss and reinforcement learning.</em></p>

<h2 id="at-long-last">At long last</h2>
<p>Wow, where to begin? The last couple of weeks have been a whirlwind but at long last i‚Äôve arrived at the end of the scholar‚Äôs program.</p>

<p>This blog post will largely be a written version of my presentation with about as much detail.
To recap, for all two of you who actually read this blog  (hi, mom üëã! ),my scholar‚Äôs project has been spent thinking about what i‚Äôll call the test time compute dream. Briefly: 
<em>Can we construct models that continuously improve their outputs the more compute we pout into them at test time?</em></p>

<h2 id="recurrence-alone-is-inadequate">Recurrence Alone is Inadequate</h2>
<p>Broadly I tend to mentally partition test time compute ideas from literature into two general categories..</p>

<ol>
  <li>
    <p><strong>Generalization Improvement mechanisms:</strong> These deal with the question of whether we can create models that leverage test time compute to learn more general algorithms instead of simple statistical association. Ideally, we‚Äôd like for to construct  algorithms that use the extra compute to resolve ambiguity or to refine their outputs using computation they‚Äôve already done or outputs they‚Äôve already produced as new inputs for future time steps. Ideally we would like to have some guarantees that these models are truly computational complete. The Universal Transformer is my canonical example of work in this category.</p>
  </li>
  <li>
    <p><strong>Efficiency Mechanisms:</strong> This largely deals with the question of whether we can decouple the amount of time it takes to run a model at inference from the number of parameters a model has. The motivation here is simple; we would like to  increase the representational capacity  of our model by increasing the total number of trainable parameters without simultaneously incurring increased computational penalty for those extra parameters.  Examples of literature of this variety includes eternal memory mechanisms (fast product keys etc)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup></p>
  </li>
</ol>

<p>By in large, this project largely focused on the former mechanism.</p>

<p>Particularly, for the first two thirds of this project I was interested in exploring the generalization properties of test time compute methods in the context of the <a href="https://github.com/Scikud/cityDataset">‚Äúshortest path dataset‚Äù</a> . I‚Äôve talked about the shortest path set <a href="https://scikud.github.io/openai/2021/01/15/road-so-far/">several times on this blog</a>, so I wont go into too much detail here. The important question being explored was whether, if we control for total training FLOP budget, does a model trained to leverage test time compute (in the form of top layer recurrence) ever approach the performance of a model that doesn‚Äôt use this recurrence but perhaps is larger or was trained for more total gradient steps overall.</p>

<p>The way I did this was by keeping the training budget fixed and then training recurrent models with a fixed number of time steps during training with loss evaluated at every time step and then evaluating those same models with more steps of recurrence at test time. What I was interested in here was whether we ever observe a phase transition whether the extra compute allows these models to catch up with larger models trained without this recurrence.</p>

<p>Obviously theres much nuance here ‚Äì the very nature this experiment has questionable foundations ‚Äì regardless, to the extent to which we can answer the  question above, the answer seems largely to be: <em>not really</em>. That is, irrespective  of the model type you attempt this scheme on (and I tried a wide array of models indeed) we never observe a phase transition where it becomes favorable to expend your training budget on training a model that uses this recurrence scheme over just training a non recurrent model for more gradient update steps.</p>

<p>Running a linear probe over the embedding space of these models reveals that they actually learn the locations of the cities fairly quickly (or at the very least something isometric to the locations). The trouble really does appear as though learning some general shortest path finding algorithm seems to be a sufficiently difficult task for all these state of the art models. Even if you argue that cross entropy loss or perplexity is a poor metric to measure performance on something like shortest path, their performance is actually even worse on more <a href="https://scikud.github.io/openai/2021/01/15/road-so-far/">sensible metrics</a>.</p>

<p>To be clear, we really don‚Äôt care <em>at all</em> about actually solving the shortest path task, it exists almost solely as a vessel to explore some of these test time compute generalization properties. The actual, absolute, performance on the dataset is largely irrelevant, what matters is the relative performance between models using test time recurrence and those models not leveraging it.</p>

<p>These negative findings hold true to domains outside of shortest path as well. Admittedly, the experiments here were much less extensive but this trend seems to be true even for tasks like sorting (even when models are trained with <em>more</em> than the \(n \log n\) recurrence steps we know are algorithmically sufficient for this domain).  It really does appear as though test time compute in the form of recurrence is insufficient to achieve the generalization properties we seek. More structure is required.</p>

<h3 id="graph-neural-networks-to-the-rescue">Graph Neural Networks to the rescue</h3>
<p>In search of this additional structure, I turned to graph neural networks.</p>
<p style="text-align: center;"><img src="/images/GNN_intro.png" alt="GNNIntroSlide" />.</p>

<p>Graph neural networks are neural networks that operate on graph structured data<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup>.</p>

<p>GNNs process this graph by iteratively performing a learned message passing operation between nodes in which the GNN attempts to refine it‚Äôs internal representation of the nodes. It does this by using a learned message passing mechanism where at each timestep the hidden state is updated in the following way:</p>

\[h_i = \phi(\, x_i, \, \bigoplus_{j \in N_i} \psi(x_i, x_j) \,)\]

<p>Above, \(h_i\) is the hidden state for some node i,  and \(x_i\) is the node embedding  for a particular node. Effectively, the update equation specifies that the hidden state at each  layer (or time step in our case) be updated by a function that takes in as inputs the node embedding, and all pairs of that nodes neighbors passed through some message passing function  \(\psi\) and then aggregated using some aggregation function.</p>

<p>The training regime was done in effectively the exact same way as the shortest model explorations where I force the model to make a prediction <em>at every timestep</em> and evaluate the loss at <em>every</em> timestep as well. This is done to ensure that the model is robust to being evaluated at <em>test</em> time with more evaluations than the model was evaluated with during training time.</p>

<h3 id="signs-of-life">Signs of Life</h3>

<p style="text-align: center;"><img src="/images/SudokuSolving.gif" alt="SudokuPlaying" />.</p>

<p>Above I embedded a movie of this GNN operating over some of the sudoku dataset. What‚Äôs particularly interesting is that it seems to prioritize using the extra test time compute (graph refinement steps) to  attend to tokens it had previously assigned high uncertainty (low probability) to in the previous time steps. In other words red things become green and green things tend to remain green. This is fascinating because it suggests that the probabilities we extract from the logits seem to have some semantically meaningful concept of uncertainty.</p>

<p>Of further interest is that fact that this GNN actually seems to do better the <em>more</em> iterations you give it. Particularly, if you evaluate it with more iterations than those it was trained with it continues to improve it‚Äôs accuracy in an almost monotonic way. Additionally, if you evaluate it on problems that are harder than the problems it was trained on, it actually still does reasonably well (check out the presentation for the actual graphs).</p>

<p>While nothing in the above is particularly <a href="https://arxiv.org/pdf/1711.08028.pdf">novel</a>, it does demonstrate that at least in principle the test time compute dream is possible. The key ingredient here seems to be related to the recurrence <em>plus</em> the message passing mechanism of these graph neural networks.</p>

<h3 id="can-we-do-better-deqs-to-the-rescue">Can we do better? DEQs to the rescue</h3>
<p>If the central argument here is <em>more test time compute in the form of iterations seems to be helpful</em> what if we could take this argument to the infinite limit. In order to do this we need to steal the machinery from deep equilibrium models. I‚Äôve written about deep equilibrium models several times before on this blog<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote">3</a></sup>, but the main take away here is that the graph refinement equation for graph neural networks is <em>exactly</em> a fixed point equation which means that the implicit function theorem allows us to use some arbitrary black box root finding algorithm to both evaluate the value of the function at the equilibrium point and the value of the gradients there as well.</p>

\[h_i = \phi(\, x_i, \, \bigoplus_{j \in N_i} \psi(x_i, x_j) \,)\]

\[g^t_i(h_i, x_i) = h^{t-1}_i - \phi(\, x^{t-1}_i, \, \bigoplus_{j \in N_i} \psi(x^{t-1}_i, x^{t-1}_j) \,)\]

<p>Trying this out, works surprisingly well, <em>at first</em>. The deep equilibrium GNN both trains much faster, seems to have better accuracy,  and has lower memory requirements than the traditional GNN.</p>

<p style="text-align: center;"><img src="/images/DEQGraphs.png" alt="SudokuPlaying" /></p>

<p>All is not well in paradise however; every single time I‚Äôve trained this model, i‚Äôve observed a strange crash in the training accuracy occur several hours in. The DEQ GNN will train with better accuracy than the traditional GNN, and then all of a sudden die wherein the accuracy plummets to zero. I‚Äôm still investigating this and while I have some suspicions about the cause (particularly instability caused by the growth of the spectral norm of the operators) it could also very well be a bug in my training loop. Either way, just mentioning these things here for the sake of completeness.</p>

<h2 id="learning-graph-adjacencies-using-policy-gradients">Learning Graph Adjacencies Using Policy Gradients</h2>

<p>All the above is well and good, however, there‚Äôs something a little strange about the way I‚Äôve seen graph neural networks used - the fact that the adjacencies must be encoded by hand. For the sudoku problem in particular, I had to explicitly bake in the fact that nodes that share a row, column or a cell are connected.  Could we instead learn the adjacencies from the raw unstructured data?</p>

<p>Here‚Äôs the idea: transformers are fairly adept at learning how relevant <em>pairs</em> of tokens are to each other. On the other hand GNNs seem to be good at performing well on relational reasoning style tasks particularly over graph structured data. What if we could use the attention head from a standard transformer to extract an adjacency matrix which we then feed into a GNN.</p>

<p>This scheme operates by first feeding our input into a small transformer which has a small modification in the top layer such that we use the probability scores to categorically sample the top K indicies which are most relevant (i.e for each input token what are corresponding <em>k</em> other tokens which the attention head assigns the highest normalized probability to). This then extracts K neighborhoods for each token which we than feed into our GNN.</p>

<p>Because sampling indices is a non differentiable operation, we need to compensate for this by using a gradient estimator. John Schulman has a great paper<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">4</a></sup> describing exactly how to do this in the general case for stochastic compute graphs.</p>

<p style="text-align: center;"><img src="/images/schulmanStochasticComputegraph.png" alt="Stochastic Compute Graph" />.</p>

<p>The formalism outlined in the paper  gives us a way to convert stochastic computation graphs into deterministic compute graphs and evaluate a ‚Äúsurrogate loss‚Äù that provides a mechanism to use standard backprop to arrive at an unbiased  gradient estimator through these stochastic nodes.</p>

<p>If you try all the above out it kind of works! Works in the sense that the model will achieve low training loss (as well as low validation loss). In reality however, there are several caveats here. The most obvious being the much slower training. Training this Frankenstein stochastic graph transformer took about 5 days compared to the 5 hours of training required for the traditional GNN. Additionally, performance is actually worse that the standard GNN ‚Äìpeaking at 62% best accuracy. Lastly, using vanilla policy gradients in this way has really high variance.</p>

<p style="text-align: center;"><img src="/images/validationLossStochasticGNNXformer.png" alt="Stochastic GNN Transformer Loss" /></p>

<p>These objections and caveats aside, the interesting thing here is that this demonstrates that <em>in principle</em> one <em>could</em> train a GNN with adjacencies learned from scratch as well. I can imagine this being useful in a number of ways, particularly in contexts where its useful to use the strong relational reasoning performance of graph neural networks but we‚Äôd like to learn the graphs dynamically or in a context dependent way.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>https://arxiv.org/abs/1907.05242¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>https://www.youtube.com/watch?v=uF53xsT7mjc Fantastic resource¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>https://scikud.github.io/openai/2020/12/20/troubles/¬†<a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>https://arxiv.org/pdf/1506.05254.pdf¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/openai/2021/04/09/wrapping-up/</guid>
                <description>
                    
                    All good things
                    
                </description>
                <pubDate>Fri, 09 Apr 2021 05:56:29 -0700</pubDate>
                <author>Kudzo Ahegbebu</author>
            </item>
        
    
        
            <item>
                <title>Crunchtime</title>
                <link>http://localhost:4000/openai/2021/02/25/feedback-transformer/</link>
                <content:encoded>
                    <![CDATA[
                    <h3 id="utility-functions">Utility functions</h3>
<p>I‚Äôve spent the past week writing some desperately neeeded utility functions to instrument the code that i‚Äôve built out as part of the scholar‚Äôs program. Largely, i‚Äôve been trying to gain a better understanding of exactly what‚Äôs going on inside these models as they operate over data continously. One lesson learned is on the importance of writing utility functions early, not only because it saves you valuable time when approaching deadlines, but also because being really explicit up front about the kind of data you would like to collect also informs what kind of experiments your‚Äôe likely to run.</p>

<h3 id="feedback-transformer">Feedback Transformer</h3>
<p>Last blog post, I mentioned I‚Äôve been playing around with implementing the feedback transformer. The principle idea in the feedback transformer is to allow low level representation in transformers to attend to previous higher level representations. This modifies the computational path of the the traditional transformer architecture and transforms it something functionally resembelling an autoregressive RNN. I wrote out a quick and dirty implementation for this (below), which I‚Äôll clean up and post on github at some point, along with the million other things I have to catch up on.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nx">MultiHeadedAttn</span><span class="p">(</span><span class="nx">nn</span><span class="p">.</span><span class="nx">Module</span><span class="p">):</span>
    <span class="nx">def</span> <span class="nx">__init__</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">Config</span><span class="p">):</span>
        <span class="k">super</span><span class="p">(</span><span class="nx">MultiHeadedAttn</span><span class="p">,</span> <span class="nb">self</span><span class="p">).</span><span class="nx">__init__</span><span class="p">()</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">c</span> <span class="o">=</span> <span class="nx">c</span> <span class="o">=</span> <span class="nx">Config</span>
        <span class="err">#</span> <span class="nx">Generates</span> <span class="nx">queries</span><span class="p">,</span> <span class="nx">keys</span><span class="p">,</span> <span class="nx">values</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">fc1</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">,</span> <span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">qkvSize</span><span class="p">)</span><span class="o">*</span><span class="nx">c</span><span class="p">.</span><span class="nx">numAttnHeads</span><span class="p">)</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">fc2</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">)</span>
        <span class="err">#</span> <span class="nx">Create</span> <span class="nx">Mask</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">register_buffer</span><span class="p">(</span>
            <span class="dl">"</span><span class="s2">causalMask</span><span class="dl">"</span><span class="p">,</span>
            <span class="nx">torch</span><span class="p">.</span><span class="nx">tril</span><span class="p">(</span><span class="nx">torch</span><span class="p">.</span><span class="nx">ones</span><span class="p">((</span><span class="nx">c</span><span class="p">.</span><span class="nx">blockSize</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">blockSize</span><span class="p">))))</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">register_buffer</span><span class="p">(</span>
            <span class="dl">"</span><span class="s2">padMask</span><span class="dl">"</span><span class="p">,</span>
            <span class="nx">torch</span><span class="p">.</span><span class="nx">ones</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">blockSize</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">blockSize</span><span class="p">))</span>

    <span class="nx">def</span> <span class="nx">forward</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">x</span><span class="p">,</span> <span class="nx">k</span><span class="p">,</span> <span class="nx">v</span><span class="p">):</span>
        <span class="nx">B</span><span class="p">,</span> <span class="nx">T</span><span class="p">,</span> <span class="nx">embdSize</span> <span class="o">=</span> <span class="nx">x</span><span class="p">.</span><span class="nx">shape</span>  <span class="err">#</span> <span class="nx">B</span> <span class="o">=</span><span class="nx">Batch</span> <span class="nx">size</span><span class="p">,</span>  <span class="nx">T</span> <span class="o">=</span> <span class="nx">numTokens</span>
        <span class="nx">h</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">c</span><span class="p">.</span><span class="nx">numAttnHeads</span>

        <span class="nx">q</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">fc1</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span>
        <span class="nx">q</span> <span class="o">=</span> <span class="nx">q</span><span class="p">.</span><span class="nx">reshape</span><span class="p">(</span><span class="nx">B</span><span class="p">,</span><span class="nx">h</span><span class="p">,</span><span class="nx">T</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="nx">k</span> <span class="o">=</span> <span class="nx">k</span><span class="p">.</span><span class="nx">reshape</span><span class="p">(</span><span class="nx">B</span><span class="p">,</span><span class="nx">h</span><span class="p">,</span><span class="nx">T</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="nx">v</span> <span class="o">=</span> <span class="nx">k</span><span class="p">.</span><span class="nx">reshape</span><span class="p">(</span><span class="nx">B</span><span class="p">,</span><span class="nx">h</span><span class="p">,</span><span class="nx">T</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="err">#</span> <span class="nx">God</span> <span class="nx">bless</span> <span class="nx">einsum</span>
        <span class="nx">attn</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">einsum</span><span class="p">(</span><span class="dl">'</span><span class="s1">bhij,bhkj-&gt;bhik</span><span class="dl">'</span><span class="p">,</span> <span class="nx">q</span><span class="p">,</span> <span class="nx">k</span><span class="p">)</span>
        <span class="nx">mask</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">unsqueeze</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nx">padMasks</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nx">repeat</span><span class="p">(</span>
            <span class="mi">1</span><span class="p">,</span> <span class="nx">h</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[...,</span> <span class="p">:</span><span class="nx">T</span><span class="p">,:</span><span class="nx">T</span><span class="p">]</span> <span class="o">*</span><span class="nb">self</span><span class="p">.</span><span class="nx">causalMask</span><span class="p">[:</span><span class="nx">T</span><span class="p">,</span> <span class="p">:</span><span class="nx">T</span><span class="p">]</span>
        <span class="nx">attn</span> <span class="o">=</span> <span class="nx">attn</span><span class="p">.</span><span class="nx">masked_fill</span><span class="p">(</span><span class="nx">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">.,</span> <span class="nx">float</span><span class="p">(</span><span class="dl">'</span><span class="s1">-inf</span><span class="dl">'</span><span class="p">))</span>
        <span class="nx">scores</span> <span class="o">=</span> <span class="nx">F</span><span class="p">.</span><span class="nx">softmax</span><span class="p">(</span><span class="nx">attn</span><span class="o">/</span><span class="nx">np</span><span class="p">.</span><span class="nx">sqrt</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nx">c</span><span class="p">.</span><span class="nx">qkvSize</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="nx">outpt</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">einsum</span><span class="p">(</span><span class="dl">'</span><span class="s1">bhij,bhjk-&gt;bhik</span><span class="dl">'</span><span class="p">,</span> <span class="nx">scores</span><span class="p">,</span> <span class="nx">v</span><span class="p">)</span>
        <span class="nx">outpt</span> <span class="o">=</span> <span class="nx">outpt</span><span class="p">.</span><span class="nx">view</span><span class="p">(</span><span class="nx">B</span><span class="p">,</span> <span class="nx">T</span><span class="p">,</span> <span class="nx">embdSize</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">self</span><span class="p">.</span><span class="nx">fc2</span><span class="p">(</span><span class="nx">outpt</span><span class="p">)</span>


<span class="kd">class</span> <span class="nx">DecoderBlock</span><span class="p">(</span><span class="nx">nn</span><span class="p">.</span><span class="nx">Module</span><span class="p">):</span>
    <span class="nx">def</span> <span class="nx">__init__</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">config</span><span class="p">):</span>
        <span class="k">super</span><span class="p">(</span><span class="nx">DecoderBlock</span><span class="p">,</span> <span class="nb">self</span><span class="p">).</span><span class="nx">__init__</span><span class="p">()</span>
        <span class="nx">c</span> <span class="o">=</span> <span class="nx">config</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">ln1</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">LayerNorm</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">)</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">attn</span> <span class="o">=</span> <span class="nx">MultiHeadedAttn</span><span class="p">(</span><span class="nx">config</span><span class="p">)</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">mlp</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Sequential</span><span class="p">(</span>
            <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="o">*</span><span class="mi">4</span><span class="p">),</span>
            <span class="nx">nn</span><span class="p">.</span><span class="nx">GELU</span><span class="p">(),</span>
            <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">ln2</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">LayerNorm</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">)</span>

    <span class="nx">def</span> <span class="nx">_setPadMasks</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">padMasks</span><span class="p">):</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">attn</span><span class="p">.</span><span class="nx">padMasks</span> <span class="o">=</span> <span class="nx">padMasks</span>
        
    <span class="nx">def</span> <span class="nx">forward</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">x</span><span class="p">,</span> <span class="nx">k</span><span class="p">,</span> <span class="nx">v</span><span class="p">):</span>
        <span class="nx">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">ln1</span><span class="p">(</span><span class="nx">x</span><span class="o">+</span><span class="nb">self</span><span class="p">.</span><span class="nx">attn</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="nx">k</span><span class="p">,</span><span class="nx">v</span><span class="p">))</span>
        <span class="nx">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">ln2</span><span class="p">(</span><span class="nx">x</span> <span class="o">+</span> <span class="nb">self</span><span class="p">.</span><span class="nx">mlp</span><span class="p">(</span><span class="nx">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="nx">x</span>


<span class="kd">class</span> <span class="nx">TinyFeedbackTransformer</span><span class="p">(</span><span class="nx">nn</span><span class="p">.</span><span class="nx">Module</span><span class="p">):</span>
    <span class="nx">def</span> <span class="nx">__init__</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">Config</span><span class="p">):</span>
        <span class="k">super</span><span class="p">(</span><span class="nx">TinyFeedbackTransformer</span><span class="p">,</span> <span class="nb">self</span><span class="p">).</span><span class="nx">__init__</span><span class="p">()</span>
        <span class="err">#</span> <span class="nx">Size</span> <span class="nx">assertions</span>
        <span class="nx">assert</span> <span class="nx">Config</span><span class="p">.</span><span class="nx">embdSize</span> <span class="o">&gt;=</span> <span class="nx">Config</span><span class="p">.</span><span class="nx">numAttnHeads</span>

        <span class="err">#</span> <span class="nx">Configuration</span> <span class="nx">stuff</span>
        <span class="nx">c</span> <span class="o">=</span> <span class="nx">Config</span>
        <span class="nx">c</span><span class="p">.</span><span class="nx">qkvSize</span> <span class="o">=</span> <span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span> <span class="c1">// c.numAttnHeads</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">config</span> <span class="o">=</span> <span class="nx">c</span>

        <span class="nb">self</span><span class="p">.</span><span class="nx">wordEmbedding</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Embedding</span><span class="p">(</span>
            <span class="nx">c</span><span class="p">.</span><span class="nx">paddingIndx</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">,</span> <span class="nx">padding_idx</span><span class="o">=</span><span class="nx">c</span><span class="p">.</span><span class="nx">paddingIndx</span><span class="p">)</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">posEmbedding</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Parameter</span><span class="p">(</span>
            <span class="nx">torch</span><span class="p">.</span><span class="nx">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">blockSize</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">))</span>

        <span class="err">#</span><span class="nx">New</span> <span class="nx">learnable</span> <span class="nx">parameters</span> <span class="k">for</span> <span class="nx">feedback</span> <span class="nx">transformer</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">memoryCoeff</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Parameter</span><span class="p">(</span><span class="nx">torch</span><span class="p">.</span><span class="nx">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">numLayers</span><span class="p">))</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">ffkv</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">,</span> <span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">qkvSize</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="nx">c</span><span class="p">.</span><span class="nx">numAttnHeads</span><span class="p">)</span>

        <span class="nb">self</span><span class="p">.</span><span class="nx">blocks</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="nx">DecoderBlock</span><span class="p">(</span><span class="nx">c</span><span class="p">)</span> <span class="k">for</span> <span class="nx">_</span> <span class="k">in</span> <span class="nx">range</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">numLayers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">ln1</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">LayerNorm</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">)</span>
        <span class="nb">self</span><span class="p">.</span><span class="nx">head</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">embdSize</span><span class="p">,</span> <span class="nx">c</span><span class="p">.</span><span class="nx">paddingIndx</span><span class="p">,</span> <span class="nx">bias</span><span class="o">=</span><span class="nx">False</span><span class="p">)</span>

        <span class="nb">self</span><span class="p">.</span><span class="nx">apply</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nx">_init_weights</span><span class="p">)</span>

    <span class="nx">def</span> <span class="nx">forward</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">indxs</span><span class="p">,</span> <span class="nx">padMasks</span><span class="p">):</span>
        <span class="k">for</span> <span class="nx">mod</span> <span class="k">in</span> <span class="nb">self</span><span class="p">.</span><span class="nx">blocks</span><span class="p">:</span>
            <span class="nx">mod</span><span class="p">.</span><span class="nx">_setPadMasks</span><span class="p">(</span><span class="nx">padMasks</span><span class="p">)</span>
        <span class="nx">numTokens</span> <span class="o">=</span> <span class="nx">indxs</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>


        <span class="err">#</span> <span class="nx">Combine</span> <span class="nx">word</span> <span class="nx">and</span> <span class="nx">position</span> <span class="nx">embeddings</span>
        <span class="nx">x</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">wordEmbedding</span><span class="p">(</span><span class="nx">indxs</span><span class="p">)</span>
        <span class="nx">pos</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">posEmbedding</span><span class="p">[:,</span> <span class="p">:</span><span class="nx">numTokens</span><span class="p">,</span> <span class="p">:]</span>
        <span class="nx">x</span> <span class="o">=</span> <span class="nx">x</span><span class="o">+</span><span class="nx">pos</span>

        <span class="err">#</span><span class="nx">Initalize</span> <span class="nx">the</span> <span class="nx">memory</span> <span class="nx">tensor</span>
        <span class="nx">memory</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">tensor</span><span class="p">([]).</span><span class="nx">to</span><span class="p">(</span><span class="nx">device</span><span class="p">)</span>
        <span class="nx">batchSize</span> <span class="o">=</span> <span class="nx">x</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>        
        <span class="nx">blockSize</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">config</span><span class="p">.</span><span class="nx">blockSize</span>
        <span class="nx">maxMemSize</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">config</span><span class="p">.</span><span class="nx">memorySize</span>
 

        <span class="nx">finalOutputs</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">tensor</span><span class="p">([]).</span><span class="nx">to</span><span class="p">(</span><span class="nx">device</span><span class="p">)</span>
        <span class="err">#</span><span class="nx">Pass</span> <span class="nx">through</span> <span class="nx">the</span> <span class="nx">transformer</span>
        <span class="k">for</span> <span class="nx">indx</span> <span class="k">in</span> <span class="nx">range</span><span class="p">(</span><span class="nx">x</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="nx">currSlice</span> <span class="o">=</span> <span class="nx">x</span><span class="p">[:,</span> <span class="nx">indx</span><span class="p">,</span> <span class="p">...].</span><span class="nx">view</span><span class="p">(</span><span class="nx">batchSize</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="nx">inpt</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">cat</span><span class="p">((</span><span class="nx">memory</span><span class="p">,</span><span class="nx">currSlice</span><span class="p">),</span> <span class="nx">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="nx">inpt</span> <span class="o">=</span> <span class="nx">inpt</span><span class="p">[:,</span> <span class="o">-</span><span class="nx">maxMemSize</span><span class="p">:,</span> <span class="p">...]</span>

            <span class="err">#</span> <span class="nx">Grab</span> <span class="nx">the</span> <span class="nx">W_k</span> <span class="nx">and</span> <span class="nx">the</span> <span class="nx">W_v</span> <span class="nx">parameters</span>
            <span class="nx">wk</span><span class="p">,</span> <span class="nx">wv</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">ffkv</span><span class="p">(</span><span class="nx">inpt</span><span class="p">).</span><span class="nx">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

            <span class="nx">outputs</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">tensor</span><span class="p">([]).</span><span class="nx">to</span><span class="p">(</span><span class="nx">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="nx">decoderBlock</span> <span class="k">in</span> <span class="nb">self</span><span class="p">.</span><span class="nx">blocks</span><span class="p">:</span>
                <span class="nx">inpt</span> <span class="o">=</span> <span class="nx">decoderBlock</span><span class="p">(</span><span class="nx">inpt</span><span class="p">,</span> <span class="nx">wk</span><span class="p">,</span> <span class="nx">wv</span><span class="p">)</span>
                <span class="nx">outputs</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">cat</span><span class="p">((</span><span class="nx">outputs</span><span class="p">,</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">unsqueeze</span><span class="p">(</span><span class="nx">inpt</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:],</span><span class="mi">0</span><span class="p">)))</span>
            <span class="nx">currmemory</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">einsum</span><span class="p">(</span><span class="dl">'</span><span class="s1">il,lbd-&gt;bd</span><span class="dl">'</span><span class="p">,</span>
                                      <span class="nx">torch</span><span class="p">.</span><span class="nx">softmax</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nx">memoryCoeff</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="nx">outputs</span><span class="p">)</span>
            
            <span class="nx">memory</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">cat</span><span class="p">((</span><span class="nx">memory</span><span class="p">,</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">unsqueeze</span><span class="p">(</span><span class="nx">currmemory</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="nx">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="nx">memory</span> <span class="o">=</span> <span class="nx">memory</span><span class="p">[:,</span> <span class="o">-</span><span class="nx">maxMemSize</span><span class="p">:,</span> <span class="p">...]</span>
            <span class="nx">finalOutputs</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">cat</span><span class="p">((</span><span class="nx">finalOutputs</span><span class="p">,</span><span class="nx">torch</span><span class="p">.</span><span class="nx">unsqueeze</span><span class="p">(</span><span class="nx">inpt</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">,:],</span><span class="mi">1</span><span class="p">)),</span> <span class="nx">dim</span> <span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="nx">finalOutputs</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nx">head</span><span class="p">(</span><span class="nb">self</span><span class="p">.</span><span class="nx">ln1</span><span class="p">(</span><span class="nx">finalOutputs</span><span class="p">))</span>

        <span class="k">return</span> <span class="nx">finalOutputs</span>

    <span class="nx">def</span> <span class="nx">_init_weights</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="nx">module</span><span class="p">):</span>
        <span class="k">if</span> <span class="nx">isinstance</span><span class="p">(</span><span class="nx">module</span><span class="p">,</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Embedding</span><span class="p">):</span>
            <span class="nx">d</span> <span class="o">=</span> <span class="p">(</span><span class="nx">module</span><span class="p">.</span><span class="nx">embedding_dim</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
            <span class="nx">module</span><span class="p">.</span><span class="nx">weight</span><span class="p">.</span><span class="nx">data</span><span class="p">.</span><span class="nx">normal_</span><span class="p">(</span><span class="nx">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nx">std</span><span class="o">=</span><span class="mf">0.125</span><span class="o">/</span><span class="nx">d</span><span class="p">)</span>
        <span class="k">if</span> <span class="nx">isinstance</span><span class="p">(</span><span class="nx">module</span><span class="p">,</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">):</span>
            <span class="nx">d</span> <span class="o">=</span> <span class="p">(</span><span class="nx">module</span><span class="p">.</span><span class="nx">in_features</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
            <span class="nx">module</span><span class="p">.</span><span class="nx">weight</span><span class="p">.</span><span class="nx">data</span><span class="p">.</span><span class="nx">normal_</span><span class="p">(</span><span class="nx">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nx">std</span><span class="o">=</span><span class="mf">0.125</span><span class="o">/</span><span class="nx">d</span><span class="p">)</span>
            <span class="k">if</span> <span class="nx">module</span><span class="p">.</span><span class="nx">bias</span> <span class="nx">is</span> <span class="nx">not</span> <span class="nx">None</span><span class="p">:</span>
                <span class="nx">module</span><span class="p">.</span><span class="nx">bias</span><span class="p">.</span><span class="nx">data</span><span class="p">.</span><span class="nx">zero_</span><span class="p">()</span>
        <span class="nx">elif</span> <span class="nx">isinstance</span><span class="p">(</span><span class="nx">module</span><span class="p">,</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">LayerNorm</span><span class="p">):</span>
            <span class="nx">module</span><span class="p">.</span><span class="nx">bias</span><span class="p">.</span><span class="nx">data</span><span class="p">.</span><span class="nx">zero_</span><span class="p">()</span>
            <span class="nx">module</span><span class="p">.</span><span class="nx">weight</span><span class="p">.</span><span class="nx">data</span><span class="p">.</span><span class="nx">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div></div>

<p>Anywho, that‚Äôs it for now. More to come in the coming weeks if I ever find time to organize my thoughts and my work. TTYL</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/openai/2021/02/25/feedback-transformer/</guid>
                <description>
                    
                    Sharing Some Code
                    
                </description>
                <pubDate>Thu, 25 Feb 2021 04:56:29 -0800</pubDate>
                <author>Kudzo Ahegbebu</author>
            </item>
        
    
        
            <item>
                <title>Updating my views</title>
                <link>http://localhost:4000/openai/2021/02/12/ml-thoughts/</link>
                <content:encoded>
                    <![CDATA[
                    <p>This past week, i‚Äôve been playing around with implementing a feedback transformer as well as improving some of the baselines i‚Äôve constructed. Lots of ongoing work on those fronts I don‚Äôt particularly feel like writing about now. While explaining some of the things I‚Äôve been working on to someone recently, I realized the way I think about the directionality of machine learning in aggregate has changed. The following post is largely a reconstruction of that conversation.</p>

<h2 id="the-current-state-of-affairs">The current state of affairs</h2>
<p>Let‚Äôs say we‚Äôd like to create a machine learning algorithm, particularly a neural network that learns some specific algorithm ‚Äì say Dijkstra‚Äôs. Now, if the idea of creating a neural network to haphazardly implement a shitty approximation of a well known existing algorithm seems contrived,that‚Äôs because it is- but stick with me I‚Äôm making a rhetorical point. Anyway, in order to carry out this task in the modern machine learning paradigm one is resigned to the somewhat tedious task of either finding or creating datasets that are sufficiently large and sufficiently expressive in the hopes of capturing enough complexity for your neural network to learn the algorithm you care about.</p>

<p>Paired with the increasing ubiquity of machine learning models<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup> themselves this seems to shift the task of learning the algorithm largely into the domain of data set engineering, i.e more specifically the problem of gathering and altering datasets (<a href="https://medium.com/@karpathy/software-2-0-a64152b37c35">Andrej‚Äôs famous Software 2.0</a>). In practice however, we usually find that the model finds it much easier to learn features specific to it‚Äôs training dataset rather than general features of the problem domain you actually care about.  Again we run into the orignal issue; in order to even learn a relatively straightforward general algorithm we‚Äôre forced to  either hamfist what we believe to be good inductive priors into our model or find/create exquisitely curated datasets that capture all the nuance we would like our model‚Äôs outputs to capture. In the context of the current machine learning paradigm, for all interesting problems it seems exceedingly unlikely that such curation is possible; there seems to be some critical <em>general</em> ingredient missing.</p>

<h3 id="what-i-thought-would-work">What I thought would work</h3>
<p>In response to this, my initial thoughts around test time compute went something like this : Maybe by more explicitly baking in compute at test time (baking in recurrence back in transformers etc), we could make it easier for our models to favor leveraging the additional computation over simply recognizing spurious patterns in the dataset. The results of the (admittedly naive) experiments I‚Äôve been running are still TBD but in general the trend seems to be that this only helps in particular contexts, and even then the benefits aren‚Äôt super pronounced.
What does seem a little clear now is that what I‚Äôve been doing thus far amounts to something like a fancy form of regularization.</p>

<h3 id="score-models">Score models?</h3>
<p>The problematic bit seems to be that using more compute at test time is only useful if your network‚Äôs learning procedure actually is able to imbue it with some mechanism to make efficient use of this additional computation.  One way to make this  explicit is to incorporate a discriminator into our generative mechanism. At the beginning of this project, I chose to not pursue this direction because at the moment it didn‚Äôt seem sufficiently general. Now however, the question that keeps popping up in the back of my head is whether it‚Äôs impossible to actually construct a model that continually refines the quality and fidelity of its outputs without it also implicitly learning something roughly isometric to a discriminator or some sort of energy minimization mechanism.</p>

<h3 id="what-does-the-future-hold">What does the future hold</h3>
<p>I‚Äôm not sure at all where to go with all of this. The asymmetry between how comparatively easy it is to recognize a good solution to some problem and the difficulty in actually generating these solutions would seem to suggest we invest our efforts into mechanisms that learn score like correspondences between inputs and outputs and then deploy search mechanisms at test time to steer themselves to  increasingly more fit outputs.  Importantly, the idea i‚Äôm trying to convey here is that if what we care about is learning how to reason, rather than learning how to solve particular datasets then  what we should care about is not the outputs themselves  but rather the search mechanisms used to operate over this score landscape. Anyway, that‚Äôs it for now. TTYL ‚úåÔ∏è</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>https://arxiv.org/abs/1706.05137¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/openai/2021/02/12/ml-thoughts/</guid>
                <description>
                    
                    idk man, stuff
                    
                </description>
                <pubDate>Fri, 12 Feb 2021 04:56:29 -0800</pubDate>
                <author>Kudzo Ahegbebu</author>
            </item>
        
    
        
            <item>
                <title>On Fixed points</title>
                <link>http://localhost:4000/openai/2021/01/29/anderson-acceleration/</link>
                <content:encoded>
                    <![CDATA[
                    <h3 id="some-thoughts">Some thoughts</h3>
<p>This week, i‚Äôve been building out an implementation of a deep equilibrium model as a means to explore some of the ideas centering around test time compute. I‚Äôve talked about deep equilibrium models<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup> briefly in <a href="https://scikud.github.io/openai/2020/12/20/troubles/">another post</a>, so I won‚Äôt spend too much time here going over them, but generally the thesis is that any multilayer neural network could be represented by a single layer weight tied recurrent net and that if we perform repeated forward iteration for most ‚Äúreal‚Äù architectures this recurrent weight tied net tends to converge to a fixed point. It‚Äôs an interesting idea, though the universality argument seems fine the expressivity and representational power of the fixed point seems to be a little dubious.</p>

<p>My interest in deep equilibrium models stems from the idea that in some sense they represent the infinite depth limit of a neural network. This makes them seemingly natural places to study the limits of test time compute. But more on that in another post. For now i‚Äôll leave you with two things:</p>

<h2 id="thing-1">Thing 1</h2>

<p>The first thought comes from thinking more about the nature of the fixed point that deep equilibrium models converge to. Something seems a little funky about this (though admittedly this is likely just a failure of intuition rather than any real failing of the paradigm). Intuitively it would seem that forcing models to converge to some fixed point limits their expressivity. This isn‚Äôt a formal argument by any means, but loosely we can imagine the only networks that converge to a fixed point are those with spectral norm &lt; 1 i.e contraction mappings that converge as a consequence of Banach‚Äôs fixed point theorem. While because of normalization I imagine most layers empirically satisfy this, it‚Äôs not too much of a leap to imagine and construct layers for network where this is not the case. Even if we consider the contraction mapping case another interesting question to ask would be if there are instances where we‚Äôre interested in some point that actually doesn‚Äôt represent a fixed point but rather is some intermediary.</p>

<p>In practice we observe that while deep equilibrium models are much more memory efficient that traditional architectures there still exists a question of why their performance isn‚Äôt dramatically better if they truly represent this ‚Äúinfinite depth limit‚Äù. Maybe the reason lies at least partially with the limitations of restricting ourselves to only the fixed point.</p>

<h2 id="thing-2">Thing 2</h2>

<p>The second thing i‚Äôd like to leave you with is this cool, (new to me) fixed point solver method called Anderson Acceleration. In traditional fixed point stuff you‚Äôre trying to solve a  \(f(c) = c\) type equation. The boring way is to just choose some initial point \(x_0\) and continue iteration with \(x_{i+1} =f(x_0)\) until you find some point where the residual \(x_{i+1} - x_{i}\) is lower than some tolerance threshold or you‚Äôve maxed out your number of allowed iterations.</p>

<p>This is fine if you‚Äôre lame and your function isn‚Äôt computationally expensive. But if iterating your function is itself a pain you‚Äôd like to find some way to accelerate the convergence to a fixed point. You could use Newton‚Äôs method which certainly has faster convergence, but then you have to compute Jacobians. However, if you‚Äôre function is computationally expensive, evaluating Jacobians is going to be computationally prohibitive as well. Again no fun.</p>

<p>Apparently, all the cool kids have been using something called ‚ÄúAnderson Acceleration‚Äù which aims to accelerate the convergence of fixed point iteration. At its core its basically like a finite difference secant method.  Instead of just using the previous guess to compute the next guess, we instead take a linear combination of the past \(m\) points. That is</p>

\[x_{k+1} = \sum^{m_k}_{i=0}(\alpha_k)_if(x_{k-m_k+i})\]

<p>With the constraint that we would just like for these alphas to minimize the residuals over the past m iterations. That is, if we define the past m residuals to be</p>

\[G = [f(x_k)-x_{k} ... f(x_{k-m+1}) -x_{k-m+1} ]\]

<p>Minimize \(\| G\alpha\|^2_{2}\) subject to the following normalization condition \(1^T\alpha = 1\)</p>

<p>Which you can transform into a linear system and solve any way you like.</p>

<p>The cool thing is that apparently in most cases Anderson acceleration not only accelerates convergence it also tends to avoids solution divergence.</p>

<p>Anywho, that‚Äôs it for now. CYOTF ‚úåÔ∏è</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>https://arxiv.org/pdf/1909.01377.pdf¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/openai/2021/01/29/anderson-acceleration/</guid>
                <description>
                    
                    A quick point
                    
                </description>
                <pubDate>Fri, 29 Jan 2021 04:56:29 -0800</pubDate>
                <author>Kudzo Ahegbebu</author>
            </item>
        
    
        
            <item>
                <title>The road so far ....</title>
                <link>http://localhost:4000/openai/2021/01/15/road-so-far/</link>
                <content:encoded>
                    <![CDATA[
                    <h3 id="what-ive-been-thinking-about">What i‚Äôve been thinking about</h3>
<p>This post will largely be a recap of the past month of trying to resolve a clearer picture of my research direction for the remainder of the scholar‚Äôs program.</p>

<p>The motivating question behind my research thus far is whether or not it possible to make more optimal use of the compute resources available at test time in order to refine the output produced by a machine learning model. To rephrase:</p>

<p><em>If we‚Äôre being clever can we create constructions such that smaller adaptive models can instead leverage test time compute to overcome the handicap of having a smaller number of learnable parameters?</em></p>

<p>For deep learning models there seems exists an asymmetry between training and inference (test time) compute. State of the art models are typically trained with large compute budgets and subsequently deployed on machines which use significantly smaller computational resources. Does this represent an inefficient underutilization of computation? Put succinctly: are there ways to remedy the asymmetry of training vs test compute?</p>

<p>Anthropomorphically, I think there‚Äôs good motivation here. After all, given more time to think, humans generally tend to produce higher quality answers- perhaps by spending those additional cognitive resources on resolving ambiguity.</p>

<p>There are a number of particularly interesting approaches to this problem some of which i‚Äôve talked about or hinted at in previous posts. Given the short time constraints of the scholar‚Äôs research program however, I think it makes sense to limit the scope of this investigation to just two potential avenues , namely the effects of adding in temporal recurrence into fast autoregressive transformers, and the effect  of adding externalized memory into existing models.  The  general idea here is that maybe the addition of temporal recurrence will enable training of transformer models which can iteratively refine their outputs (at least in language modeling contexts) while simultaneously decreasing sampling complexity and alleviating the per parameter computational cost increases seen in typical depth-wise recurrent architectures.</p>

<h3 id="a-detour-into-city-paths">A detour into city paths</h3>

<p>Both as a mechanism for investigating test time compute, and as a ‚Äúfail fast‚Äù test i‚Äôve constructed a dataset that‚Äôs turned out to be surprising in a number of ways.  For now what i‚Äôm just calling ‚ÄúThe City Dataset‚Äù (Github link forthcoming I swear) was made by constructing a DAG from publicly available data about US population centers. It includes some basic demographic information as well as GPS coordinates for almost 30,000 distinct cities and municipalities distributed across the nation. Besides serving as a traversable knowledge graph, this DAG allows us to also construct simpler smaller datasets.</p>

<p>Particularly, one of these derivative datasets is the ‚ÄúShortest Path Dataset‚Äù (SPD). SPD is constructed using the coordinates found in the City Dataset and uses Dijkstra‚Äôs to enumerates several million ‚Äúshortest paths‚Äù between pairs of US cities.  I treat this a sequence modeling task using a scheme akin to traditional language modeling where the network is asked to given a sequence of inputs and asked at each timestep to predict the next token (city) in the sequence target. A RNN was constructed as baseline (from which we could later compare other models trained with similar levels of compute but utilizing increased compute at test time).</p>

<p style="text-align: center;"><img src="/images/cityRNNPerformance.png" alt="iterativeModel" /></p>

<p>Above, I plot performance for different sized architectures.  Notice that training loss tends to bottom out on the order 0.3~0.4. Because cross entropy loss isn‚Äôt a particularly useful metric in this case I constructed other metrics that measure the performance of the model along certain axes. Namely:</p>
<ol>
  <li>Do the generated output paths lack illegal jumps? Here Illegal jumps are defined as jumps between two cities that are over the ‚Äúconnected distance‚Äù used to generate the DAG - the distance below which we consider two cities to be connected.</li>
  <li>Is the length of the total path traversed by the generated output bounded? In other words, this metric checks to ensure that even if we don‚Äôt generate the exact optimal path, the path that we do generate deviates from the optimal path only by a small amount.</li>
  <li>Does the generated output actually end in the same city as the optimal path?</li>
</ol>

<p>What‚Äôs remarkable is that even with a somewhat  decent training loss, performance with regard to the these other metrics is abysmal. It‚Äôs still a bit early to say definitively but as far as I can tell, even with low NLL loss generating a sample output that satisfies even one of these metrics is <em>exceedingly, exceptionally</em> rare.</p>

<p>Despite it‚Äôs artificiality, it‚Äôs important to remember it‚Äôs not actually solving this particular task itself I care about.  What was important here was finding a domain that was sufficiently challenging where the benefits of leveraging compute at test time would be clear. Shortest Path seems attractive because any traditional single pass architecture should be algorithmically bounded in how well it should perform on this. Additionally, as an example of test time compute it‚Äôs easy to imagine a model that first generates a path that‚Äôs directionally correct and then continually refines its output. What‚Äôs a bit surprising about this task is how difficult its actually proven to be even with respect to generating just decently-performing baselines.</p>

<h3 id="to-be-continued">To be continued</h3>

<p>There‚Äôs quite a bit going through my head currently. I‚Äôm devising better means to instrument what‚Äôs going on with this dataset as well as mechanisms to actually extract performance with increases in test time compute from other models using other paradigms. Above all, while i‚Äôm bullish on the idea of test time compute, I still have deep doubts about the kludgyness and crudity of my methods. I would like to leave you with something more cogent, more coherent, and more insightful but for now i‚Äôm stumbling around in the darkness. Anyway, that‚Äôs it for now. Catch you on the flip.</p>


                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/openai/2021/01/15/road-so-far/</guid>
                <description>
                    
                    A summary
                    
                </description>
                <pubDate>Fri, 15 Jan 2021 04:56:29 -0800</pubDate>
                <author>Kudzo Ahegbebu</author>
            </item>
        
    
        
            <item>
                <title>I may need to rethink things...</title>
                <link>http://localhost:4000/openai/2020/12/20/troubles/</link>
                <content:encoded>
                    <![CDATA[
                    <p>One of the particularly fascinating things i‚Äôve encountered during NEURIPS 2020 was a workshop on deep implicit layers that raises some questions about the nature of my current project proposal for the OpenAI scholars program. While I highly recommend checking out the <a href="http://implicit-layers-tutorial.org/">tutorial</a> and working through the main ideas for yourself, I‚Äôll try to super briefly outline the principle argument and method. All figures in this post are <a href="https://arxiv.org/pdf/1909.01377.pdf">stolen from their work</a>.</p>

<p>Deep equilibrium models are built on top 3 ideas.</p>
<ol>
  <li>Implicit layers are more expressive than explicit layers. That is, instead of having layers that express how to calculate a model‚Äôs output from its input we can instead specify which conditions we would like for a model‚Äôs output and input to jointly satisfy. This has a number of attractive properties (all worthy of an entire blog post) but prime among them is the nice property of decoupling a model‚Äôs solution from the procedure that generates it.</li>
  <li>Feedforward models can be represented by an equivalent, weight-tied, recurrent back-propagation model which they call a deep equilibrium model.</li>
</ol>

<p style="text-align: center;"><img src="/images/iterativeModel.png" alt="iterativeModel" /></p>
<ol>
  <li>Deep equilibrium models can be constructed such that they converge to a fixed point. What‚Äôs particularly dope is that this property combined with the decoupling property talked about above ultimately lets you plug in your models dynamics into any black box optimizer.</li>
</ol>

<p>The upshot, is that the <a href="https://en.wikipedia.org/wiki/Implicit_function_theorem">implicit function theorem</a> gives a really nice way of computing gradients near fixed points without having to store the usual intermediaries you would if you were computing and storing the computational graph using standard auto-differentiation. This allows for some fairly massive reductions in the memory requirements for equivalently sized models.  What‚Äôs interesting is to measure the performance of DEQ models when compared to more traditional single pass feed forward models.</p>

<p style="text-align: center;"><img src="/images/deqPerformance.png" alt="iterativeModel" /></p>

<p>In some sense these deep equilibrium models can be thought of as deep networks with infinite depth since they‚Äôre iteratively evaluated until their output reaches convergence up to some tolerance threshold. The troubling question this raises for the research and my project proposal research is whether a DEQ represents the limits of what‚Äôs possible vis-√†-vis iterative refinement. To rephrase, if the principal idea in my work is that it may be possible to continually and iteratively improve performance of smaller models by more optimally leveraging test time compute, then the fixed point of DEQ models may represent the upper bound of iterative application.</p>

<p>To be sure, even if DEQs do represent some performance limit for fixed parameter models there still exist fascinating questions to be asked here nonetheless. Why are the performances gains so marginal? How does the performance of DEQ models change with task algorithmic complexity? <em>etc, etc‚Ä¶</em></p>

<p>To be honest, I‚Äôm still not sure how to grapple with any of these issues, or whether these are signs that perhaps I should consider alternative proposals. For now, my two pronged approach is as follows</p>

<ol>
  <li>
    <p><strong>Fail Fast Tests:</strong> Make these questions more concrete by examining the performance of recursive transformers and external memory models on tasks we believe test time compute will actually make a difference.  Thus far i‚Äôve constructed  a set of simple algorithmic tasks (i‚Äôll soon make these available on github). The motivation here is to quickly gauge for signs of life in this work.</p>
  </li>
  <li>
    <p><strong>Explore EBM backup plan</strong> As far as I can tell, test time compute paradigm actually seems to be more at home in the energy based model framework where the idea is that you characterize an energy function that parametrizes the correspondence between your input and output. At inference, you then search for an output that minimizes the energy function (using gradient descent if the latent space is continuous) using any standard optimization technique of choice. Consequently, as a backup I‚Äôm also reviewing the EBM literature to build familiarity with this space.</p>
  </li>
</ol>

<p>Anywho that‚Äôs enough rambling for now. Catch you on the flip.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/openai/2020/12/20/troubles/</guid>
                <description>
                    
                    ruh roh
                    
                </description>
                <pubDate>Sun, 20 Dec 2020 04:56:29 -0800</pubDate>
                <author>Kudzo Ahegbebu</author>
            </item>
        
    
        
            <item>
                <title>No, less isn't more, more is more?</title>
                <link>http://localhost:4000/openai/2020/12/04/compute/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Over the past two weeks, I‚Äôve been thinking more about which research direction I‚Äôd like to pursue for the remainder of the scholar‚Äôs program. In this post ,I‚Äôd like to just outline just the motivating question without much exposition about my approach. I don‚Äôt want to spend too much time talking about my research direction just yet largely because I‚Äôm still searching for mechanisms to think more clearly about these issues.</p>

<p><strong>Broad context</strong>. One thing in particular that‚Äôs struck me (as an effect of being witness to some of the work being undertaken at OpenAI) is a massive asymmetry in compute budgets. State of the art models are typically trained on massive multi-GPU clusters and then subsequently deployed on machines with significantly smaller computational capacity. While this may be desirable for edge or on-device computing, an interesting question to ask is whether or not this represents an inefficient underutilization of the computational resources.</p>

<p><em>If we‚Äôre being clever can create constructions such that smaller adaptive models can instead leverage test time compute to overcome the handicap of having a smaller number of learnable parameters?</em></p>

<p>Broadly, I actually don‚Äôt think that simply scaling learning models will lead to the most qualitative gains in the expressiveness and generality of machine intelligence. However, in accordance to the <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">bitter lesson</a> thinking about mechanisms that allow all the computation available to us to be more optimally be allocated is certainly a deeply useful endeavor.</p>

<p>Generally, I believe there‚Äôs a great deal of utility in thinking more explicitly about compute budgets as a fundamental part of the broader optimization problem we attempt to solve when constructing machine learning models. Explicitly, given a fixed computational budget how do we optimize that budget between our training and test time regimes?  This question takes increasing precedence when you operate at the scale of OpenAI.  In some sense active learning is one perspective to approach this question (e.g which subsets of the internet should you train your GPT-X model, keeping in mind that determining those subsets also comes from your compute budget).  More speculatively, other approaches seem to indirectly relate to this question of  iterative improvement/test time compute as well, particularly Hebbian learning approaches, or latent variable energy models.</p>

<p>Anywho, that‚Äôs all for now. Catch you on the flip.</p>

<p>‚Äì</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/openai/2020/12/04/compute/</guid>
                <description>
                    
                    Compute maximalism
                    
                </description>
                <pubDate>Fri, 04 Dec 2020 04:56:29 -0800</pubDate>
                <author>Kudzo Ahegbebu</author>
            </item>
        
    
        
            <item>
                <title>Reframe/Reparametrize</title>
                <link>http://localhost:4000/openai/2020/11/22/reframe-reparametrize/</link>
                <content:encoded>
                    <![CDATA[
                    <p>In continuation of the tradition of this blog, where I start off writing a post about one topic and ultimately end up deleting it a quarter of the way through and begin writing about something totally different, I present to you this week‚Äôs topic: reframing and reparametrization using Lagrange multipliers.</p>

<p>One tool i‚Äôve found particularly useful throughout the years is taking problems I have in one domain and trying to reframe them as an optimization problem in order to use the method of Lagrange multipliers to arrive at a solution. The utility of <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a> (and more broadly <a href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">the KKT conditions</a> ) is that as long as you can frame what you care about as some sort of constrained optimization problem, the method of lagrange multipliers gives you a simple but powerful method to derive algorithms as well as understand and bound your problem‚Äôs sensitivites. I‚Äôve always found this to be really cool, because at it‚Äôs core you simply specify something you‚Äôd like to optimize (even if it‚Äôs non-convex), specify contraints that thing should ideally satisfy and like magic, out pops an algorithm or a function corresponding to the optimum (or if not you usually at least get computable conditions that optimum should satisfy).</p>

<p>I‚Äôll walk through 3 examples of this type of constrained optimization that have popped up for me over the past month in the context of machine learning. Namely, using the Lagrange formalism as one perspective on how to  arrive at the origins of the Gaussian distribution, the Backprop algorithm, and the Trust Region Policy Optimization (TRPO) algorithm in reinforcement learning (in some ways the predecessor of the more widely used PPO algorithm).</p>

<h2 id="origins-of-the-gaussian-maximum-entropy-distributions">Origins of the Gaussian (Maximum Entropy Distributions)</h2>
<p>A natural question to ask is: What‚Äôs the most general distribution one can use to describe random variables with finite mean and finite variance?</p>

<p>The entire point of statistics is to infer both the shape and the parameters that control the underlying distributions generating our samples. What‚Äôs the best we can do? Considering the space of all possible distributions we could potentially choose from we‚Äôd like to find a distribution that satisfies these constraints as weakly as possible. Having as little pre-existing structure as possible ensures it remains as general as possible, making the ‚Äúleast claim of being informed beyond the stated prior data‚Äù. What we‚Äôre really saying  is that we want a distribution that maximizes entropy<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>.</p>

<p>\(\text{maximize}[H(x)] = \text{maximize}[\int_{-\infty}^{\infty}p(x)\log(P(x)dx ]\).</p>

<p>To reiterate the distribution must also satisfy the finite mean and finite variance constraints, explicitly:</p>

<ul>
  <li><strong>Constraint 0</strong> :Probability density function must sum to 1:  \(\int_{-\infty}^{\infty}p(x)dx = 1\)</li>
  <li><strong>Constraint 1</strong> :Finite mean:  \(\int_{-\infty}^{\infty}p(x)xdx = \mu\)</li>
  <li><strong>Constraint 2</strong> :Finite variance: \(\int_{-\infty}^{\infty}p(x)\left(x-\mu\right)^2dx = \sigma\)</li>
</ul>

<p>Putting in all together using the method of Lagrange multipliers we have:</p>

\[\begin{align}
\mathcal{L}= 
\int_{-\infty}^{\infty}p(x)\log p(x)dx 
-\lambda_0\left(\int_{-\infty}^{\infty}(p(x)-1\right)dx 
-\lambda_1\left(\int_{-\infty}^{\infty}p(x)(x)dx-\mu\right) \\
-\lambda_2\left(\int_{-\infty}^{\infty}p(x)(x-\mu)^2dx-\sigma\right) 
\end{align}\]

<p>Since \(\mathcal{L} = \mathcal{L}(P(x), \lambda)\), taking the gradients we have :</p>

\[\begin{align}
\frac{\partial \mathcal{L}}{\partial p(x)} =  \log p(x) +1 - \lambda_0 - \lambda_1x -\lambda_2(x-\mu)^2 = 0 \\
\frac{\partial \mathcal{L}}{\partial \lambda_0}  = \int_{-\infty}^{\infty}p(x)dx-1 = 0\\\
\frac{\partial \mathcal{L}}{\partial \lambda_1} = \int_{-\infty}^{\infty}p(x)xdx-\mu = 0 \\
\frac{\partial \mathcal{L}}{\partial \lambda_2}  = \int_{-\infty}^{\infty}p(x)\left(x-\mu\right)^2dx-\sigma = 0 \\
\end{align}\]

<p>I won‚Äôt put the algebra here (it‚Äôs not at all interesting and you probably shouldn‚Äôt much care) but you start with</p>

\[\log p(x) = \lambda_0 + \lambda_1 x + \lambda_2(x-\mu)^2 -1\]

<p>keep cranking and you‚Äôll eventually end up with the following:</p>

\[P(x) = \frac{1}{\sqrt{2\pi \sigma^2}} exp\left(-\frac{(x-mu)^2}{2 \sigma^2}\right)\]

<p>The Gaussian distribution at last. To recap, by framing our original question as a constrained optimization problem we were able to retrieve the gaussian distribution in a natural sort of way.</p>

<h2 id="origins-of-backprop">Origins of Backprop</h2>
<p>It turns out we can reframe back-propagation in a <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-88.pdf">similar fashion</a>. Ultimately, supervised learning is about minimizing the error between some labeled data \(Y\) and our model‚Äôs predictions, usually given by the last layer of our neural network \(Z^L\).</p>

<p>Compactly: minimize \(Loss(z^L, y)\), where \(z^{i} = f^i(z^{i-1},W^i)\)</p>

<p>At it‚Äôs core, it‚Äôs just another instance of a constrained optimization problem that can be tackled with the same Lagrangian formalism as before.</p>

\[\mathcal{L}(z, W, \lambda)=  Loss(z^L, y) -\sum\limits_{i=1}^L \lambda_{i}^T\left(z^i - f^i(z^{i-1})\right)\]

<p>Above, we express the dynamics of each layer of the network as a sum through all \(L\) layers. Effectively, all we‚Äôre saying is ‚Äìminimize some loss function  <em>S‚Äôil vous pla√Æt</em> ‚Äìwith the constraint that each layer \(Z^i\) of our network is just a function of the previous layer \(Z^{i-1}\) through some non-linear function \(f\).</p>

<p>Taking the gradient we arrive at the following:</p>

\[\begin{align}
\nabla_\boldsymbol{\lambda_i} \mathcal{L} = z^i - f^i(z^{i-1},W^i) = 0 \\
\nabla_\boldsymbol{w} \mathcal{L} = -\sum\limits_{i=1}^L \lambda_i^T \nabla_W f^i(z^{i-1},W^i) = 0 \\ 
\nabla_\boldsymbol{z^i} \mathcal{L} = \lambda_i - \nabla f^{i+1}(z^i,w^{i+1}) = 0 \\
\nabla_\boldsymbol{z^L} \mathcal{L} = \lambda_L - \nabla Loss(z^L, y) = 0 \\
\end{align}\]

<p>Taking a looking at above, one gets almost for free the following: 
\(z^i =f^i(z^{i-1}\) and \(\lambda_L = \nabla_{z^L} Loss(z^L,y)\)</p>

<p>Additionally, Working through these (particularly the third equation)  one eventually arrives at</p>

\[\lambda_j = \sum_{i \in \beta(j)} \lambda_i \frac{\partial f_i (z^i, W^i)}{\partial z^j}\]

<p>where \(\beta(j)\) is the set of all incoming edges from the vertex j<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup></p>

<p>Which is recognizable in the context of backprop as the equation for the the adjoints \(\lambda_ i\), telling us how to measure the sensitivity of one node relative to the previous layer.  Cool.</p>
<h2 id="origins-of-trpo">Origins of TRPO</h2>
<p>Man, bad news homie ‚Äì this post is already much longer than I originally intended (and I‚Äôm getting <em>real</em> tired to typing out LaTex), so maybe I won‚Äôt go into as much detail about this one. Anyway, It turns out one can arrive at the core of <a href="https://arxiv.org/pdf/1502.05477.pdf">TRPO</a> algorithm in reinforcement learning under a similar scheme. I‚Äôll try to lay out the crux briefly:</p>

<p><a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">Reinforcement learning</a> fundamentally differs from supervised learning in some important ways. Particularly, in the supervised learning case your samples are drawn i.i.d from some underlying distribution and therefore it ultimately doesn‚Äôt really matter if you sample from a ‚Äúbad‚Äù (low reward) region of your parameter space in one batch, because your next batch will not be conditioned on the poor performing batch in any way. The issue in reinforcement learning<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">3</a></sup> is that this sampling i.i.d assumption no longer holds, precisely because your policy ultimately also <em>controls</em> your sampling process; your policy both learns from your samples and is responsible for gathering new samples.</p>

<p>Ultimately we‚Äôd only like to trust our policy within some small region. Ostensibly, a natural way to do this is to bound our parameter updates such that they stay within some small radius of their original values.</p>

\[\theta' \leftarrow  \arg\max_{\theta} (\theta' - \theta)^T \nabla J(\theta)\]

\[s.t \quad ||\theta' - \theta || ^2 \leq \epsilon\]

<p>In reality, however we don‚Äôt have much reason to expect this to be meaningful since some parameters might change much more quickly or slowly than others. Ultimately what we care about isn‚Äôt <em>really</em> that the parameters that control out policy be bounded from one update to the next, it‚Äôs that change in the policy <em>itself</em> be bounded.</p>

<p>We‚Äôd like to find a way to restrict our policy updates that isn‚Äôt explicitly dependent on whatever parametrization we used. One way to do this is instead of restricting the gradients, we  should restrict the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL Divergence</a> between the policy and it‚Äôs update to be bound by some epsilon ball. Sure sounds like a constrained optimization problem don‚Äôt it? üòâ</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Worked out in more detail <a href="https://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/">here</a> for the interested¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>At least in the on-policy case¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/openai/2020/11/22/reframe-reparametrize/</guid>
                <description>
                    
                    Another post courtesy of ya boy Lagrange
                    
                </description>
                <pubDate>Sun, 22 Nov 2020 04:56:29 -0800</pubDate>
                <author>Kudzo Ahegbebu</author>
            </item>
        
    
        
            <item>
                <title>Chasing the White Rabbit</title>
                <link>http://localhost:4000/openai/2020/11/05/ay-dios-mio/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Election week 2020 has been one of the longest 56 years of my life. Trying to get anything substantial accomplished in this context has been a humbling reminder of the limits of my will.</p>

<p>The backdrop of chaos and existential dread not withstanding, in the past two weeks I‚Äôve begun wrestling more seriously with many of the same questions I imagine all early stage researchers find themselves struggling with.</p>

<p><em>How do I efficiently allocate my time? What are the right problems? What subset of those are amenable to tractable progress?</em></p>

<p>Of course, responses to the core of how to address these questions have been crafted by a multitude of perspectives from many talented writers and research scientists<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>. <em>Intellectually</em> I think I hear what they‚Äôre saying, however, I‚Äôm still not quite sure I yet <em>grok</em> what they mean.</p>

<h3 id="rabbit-holes">Rabbit Holes</h3>
<p>For my cohort these questions take increasing saliency as our project selection phase approaches. 
In the midst of this, one of the particularly frustrating tactical aspects of the past two weeks has been being stuck in rabbit holes. Frustrating, because attempting to align some foundational concept with your intuition  seems both educationally productive yet in absolute terms unduly time consuming. It‚Äôs difficult to decipher what the correct middle ground should be. After-all, wrestling deeply with problems and learning to get yourself unstuck is a critical skill. I‚Äôve talked to a couple people about this and through these discussions two major themes recur.</p>

<ol>
<li> <em>Use the resources of those around you:</em> One thing I've come to quickly realize is how ineffectively I'm leveraging the incredible talent and resources in my proximity. I need to become better at asking more questions to more people more frequently. There is something to be gained by learning to trod through problems independently, however it makes no sense to not take full advantage of the mentorship and knowledge base that's so readily accessible</li>

<li> <em>Keeping context in mind, learn to work with imperfect information</em> . The tendency to dive into rabbit holes is one I think is born from a desire to have perfect domain information. While obviously such an impulse can be helpful and even motivating, one of perhaps even more utility is the ability to contextualize and bound your uncertainty around particular concepts in order to ensure you don't allow relatively minor detail to occlude your view of the bigger picture. 
 </li>

</ol>

<h2 id="what-ive-been-thinking-about">What I‚Äôve been thinking about</h2>
<p>Originally I actually planned to write this blog post as an attempt to breathe coherence into my thoughts on some linked concepts relating different research directions i‚Äôve been thinking about. Briefly, i‚Äôve been thinking about meta learning and operations that bake in inductive bias and various cognitive priors onto the latent space of attention models. Particularly mechanisms that would allow us to have weak formal guarantees on the  outputs produced by these models.  However, I‚Äôm not quite sure my thoughts are yet as organized on that front as i‚Äôd like for them to be. I‚Äôd like to come back and write about that some other time.</p>

<p>By the way this entire post was written by GPT3. Just playing. Catch you on the flip!</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Hamming‚Äôs <a href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.pdf">‚ÄúYou and Your research‚Äù</a> and  Schulman‚Äôs <a href="http://joschu.net/blog/opinionated-guide-ml-research.html">‚ÄúOpinionated Guide to ML Research‚Äù</a> come to mind¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/openai/2020/11/05/ay-dios-mio/</guid>
                <description>
                    
                    Searching for lightbulbs
                    
                </description>
                <pubDate>Thu, 05 Nov 2020 04:56:29 -0800</pubDate>
                <author>Kudzo Ahegbebu</author>
            </item>
        
    
        
            <item>
                <title>OpenAIs Scholars Initial Thoughts</title>
                <link>http://localhost:4000/openai/pixylls/jeykll/2020/10/22/new-blog-who-dis/</link>
                <content:encoded>
                    <![CDATA[
                    <p>I‚Äôm starting a new blog<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>  to mark the beginning of my time as part of the OpenAI scholar‚Äôs program</p>

<h2 id="things-im-excited-about">Things I‚Äôm excited about</h2>
<p>I‚Äôm deeply excited and humbled to work with such an incredible and deeply talented pool of individuals, both within my cohort and within OpenAI in general. The first week has been such a whirlwind of excitement. I‚Äôve spent much of it shoring up my foundations in machine learning theory and application as well tyring to find my bearings in the wide sea of research interests.</p>

<p>I joined this program because of my belief that well aligned machine intelligence can act as an incredible force multiplier for a wide range of human endeavors. I‚Äôm most excited by the incredible generality of deep learning; the remarkable ability to abstract and  apply the same set of algorithms to a wide range of seemingly disparate problems. This paradigm excites me because it  reimagines computers and computation as being more than numerical automata but as robust tools capable of ingesting and producing rich inputs and outputs allowing us to augment our own problem solving capacity and by extension amplify ingenuity and improve the human condition. Honestly, what could be more exciting or compelling as a research interest?</p>

<h2 id="ramblings">Ramblings</h2>
<p>One thing I‚Äôve personally found useful is keep running track of ideas that emerge during the course of self study. I‚Äôve found this idea tracking helpful in the meta-sense; in that, over time it helps me see emerging trends in my train of thought. I‚Äôm really not sure how useful these stream of consciousness type expositions will be for others, but nevertheless here are some of my thoughts from this week:</p>

<ol>
  <li>
    <p>Training a neural network produces a model that we can think of as some object whose weights and biases provide discrete estimates of the high dimensional manifold our training data lies on. Along this train of thought, can we  think of the hyperparameters of two different networks trained on the same data as two different samples of the same underlying ‚Äòtrue‚Äô manifold? What then is the relationship (if any) between the weights of these two models? Is it possible to be clever and somehow combine these two manifold samplings to get a better estimate of the true manifold ala something loosely analogous to <a href="https://francisbach.com/richardson-extrapolation/">Richardson Extrapolation</a> /Bayesian updating ?</p>
  </li>
  <li>
    <p>When training neural networks we typically include as part of the loss function, a regularization term. The idea behind this is that penalizing large model weights we can prevent the co-adaptation of network weights and limit the networks ability to learn local noise in the underlying training data thus leading to better generalization.  The regularization terms i‚Äôve seen thus far are fairy straightforward l1 or l2 norms on the network weights. I‚Äôm curious to explore the effectiveness of regularizing with more sophisticated techniques. Like regularizing with the goal to minimize entropy or something along that vein.</p>
  </li>
</ol>

<p>That‚Äôs it for now. Catch you all later!</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>For the literally two people who read the single post in the old blog, I‚Äôve migrated that post as well :)¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/openai/pixylls/jeykll/2020/10/22/new-blog-who-dis/</guid>
                <description>
                    
                    New blog who dis?
                    
                </description>
                <pubDate>Thu, 22 Oct 2020 05:56:29 -0700</pubDate>
                <author>Kudzo Ahegbebu</author>
            </item>
        
    
        
            <item>
                <title>Non-inertial Reference Frames and Coin Wishing Wells (Part I)</title>
                <link>http://localhost:4000/openai/pixylls/jeykll/2019/11/18/coin-wishing-well/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Sometime ago, a friend of mine and I were curious about the physical principles that cause coins to sprial so elegantly down the kinds of coin funnels one might find at the  local mall.
After searching online we were unable to find a description that was both mathematically satisfying and sufficiently captured the relevant physics. Having some free time, we decided to tackle this problem on our own</p>

<h3 id="lets-talk-lagrangians">Let‚Äôs talk Lagrangians</h3>

<p>Loosely speaking, in physics , whenever one is working on a trajectory problem that
involves non-cartesian coordinates it‚Äôs often easier to find the equations of motion
using the Lagrangian framework, which (as we‚Äôll see for this problem) is far more convenient for solving
problems that aren‚Äôt so well suited for Newton‚Äôs formulation of classical mechanics.</p>

<p>In particular, the utility of  Lagrange‚Äôs framework is that it allows for us to work
in the so called <em>generalized coordinates</em>. The principle being that if we‚Äôre being clever,
these generalized coordinates can (and indeed should) be chosen to exploit geometry or symmetries
present in the system we wish to analyze.</p>

<p style="text-align: center;"><img src="/OldBlog/Wishing_Well.jpeg" alt="wishingWell" width="400px" />.</p>

<p>Looking at this image of a coin well cross section, we can imagine that it might be
useful to describe the motion of a coin about the funnel in terms of it‚Äôs radius \(\hat{r}\) from the center  of the funnel,
it‚Äôs azimuthal angle \(\hat{\theta}\), and it‚Äôs height \(\hat{z}\) within the well.</p>

<p>Above, the Lagrangian is a function of kinetic and potential energy. For the kinetic energy we have:</p>

\[T(x,y,z)= \frac{1}{2}m(\dot{x}^2+\dot{y}^2 +\dot{z}^2)  \leftrightarrow  T=\frac{1}{2}m(\dot{r}^2+r^2\dot{\theta}^2+\dot{z}^2)\]

<p>the second expression for kinetic energy \(T\) follows since we have 
\(x = rcos(\theta)\) and \(y= rsin(\theta)\).
Then, since the height \(z\) is a function of the radius \(r\) we have</p>

\[\dot{z}(r,\dot{r}) = \frac{c_1\dot{r}}{r^2} \leadsto T=\frac{1}{2}m(\dot{r}^2+r^2\dot{\theta}^2+\frac{c_1^2\dot{r}^2}{r^4})\]

<p>The potential energy for this system comes simply as a result of gravity. From kindergarten we have,</p>

\[U(z) = mgz \rightarrow  U(r) = mg(-\frac{c_1}{r}+c_2)\]

<p>Therefore, we may write the lagrangian function \(\mathcal{L}=T-U\) as</p>

\[\mathcal{L}=\frac{1}{2}m(\dot{r}^2+r^2\dot{\theta}^2+\frac{c_1^2\dot{r}^2}{r^4}) - g(\frac{c_1}{r}+c_2)\]

<p>At this point, before proceeding with Lagrange‚Äôs equation, it‚Äôs important to note that expression above doesn‚Äôt
account for non-conservative forces (like friction) that dissipate the total energy of the coin as it proceeds down the well.
It could be useful to model these viscous effects using the seemingly reasonable assumption that these frictional
forces scaled proportionally with radial and angular velocity It turns out afterward, that this is actually a damn fine first order approximation</p>

<p>As an educated guess one could assume that this dissapative force might look something like</p>

\[F = \frac{1}{2}(k_r\dot{r}^2+k_{\theta}\dot{\theta}^2)\]

<p>Where \(k_r\) and \(k_{\theta}\) are just some dissipative constants. This is the so called
<a href="https://en.wikipedia.org/wiki/Rayleigh_dissipation_function">Rayleigh dissipation function</a></p>

<p>For the radial component ,the Lagrange Equation (taking into account the dissipative friction effects we might expect the coin well to follow) may be written as</p>

\[\frac{\partial \mathcal{L}}{\partial r}-\frac{\partial \mathcal{L}}{\partial t}\frac{\partial \mathcal{L}}{\partial \dot{r}} = \frac{\partial F}{\partial \dot{r}}\]

<p>After substituting the lagrangian  function $\mathcal{L}$ into the expression above, and after some tedious algebra we finally arrive at our first equation of motion</p>

\[\boxed{\ddot{r}(\frac{r^4+c_1^2}{r^4})-\frac{2c_1^2\dot{r}^2}{r^5}+\frac{gc_1}{r^2}-r\dot{\theta}^2=\frac{k_r\dot{r}}{m}}\]

<p>Bitchin‚Äô!</p>

<p>Doing precisely the same thing for the \(\theta\) component, we arrive at the second equation of motion</p>

\[\boxed{-(r^2\ddot{\theta}+2r\dot{r}\dot{\theta})=k\dot{\theta}}\]

<p>If you‚Äôre unfamiliar with Lagrangian mechanics, allow me to take a moment to explain why this result is significant:
The complete equations of motion for this system were obtained <em>despite</em> the fact that we <em>only</em> specified the kinetic and potential energies of the coin without any
reference to Newtonian theory i.e we were able to do this without ever explicitly considering the <em>forces</em> acting on the coin.</p>

<h3 id="experiments-and-simulations">Experiments and Simulations</h3>
<p>I took the equations of motion above and plugged the differential equations above into MATLAB in order to simulate the coin‚Äôs trajectory.</p>

<p style="text-align: center;"><img src="/OldBlog/test_animate.gif" alt="simulation" /></p>

<p>Taking just a cursory qualitative look at the general form of the trajectory looks remarkably like the trajectories one sees in the actual coin wells.
In the next part of this article, we‚Äôll take a look at how we can get more quantitative confidence that what we‚Äôve derived corresponds to reality. Stay tuned!</p>

<hr />

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/openai/pixylls/jeykll/2019/11/18/coin-wishing-well/</guid>
                <description>
                    
                    Exploration from the old blog
                    
                </description>
                <pubDate>Mon, 18 Nov 2019 04:56:29 -0800</pubDate>
                <author>Kudzo Ahegbebu</author>
            </item>
        
    
  </channel>
</rss>
