<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ForceMultiplied</title>
    <description>.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Reframe/Reparametrize</title>
        <description>&lt;p&gt;In continuation of the tradition of this blog, where I start off writing a post about one topic and ultimately end up deleting it a quarter of the way through and begin writing about something totally different, I present to you this week‚Äôs topic: reframing and reparametrization using Lagrange multipliers.&lt;/p&gt;

&lt;p&gt;One tool i‚Äôve found particularly useful throughout the years is taking problems I have in one domain and trying to reframe them as an optimization problem in order to use the method of Lagrange multipliers to arrive at a solution. The utility of &lt;a href=&quot;https://en.wikipedia.org/wiki/Lagrange_multiplier&quot;&gt;Lagrange multipliers&lt;/a&gt; (and more broadly &lt;a href=&quot;https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions&quot;&gt;the KKT conditions&lt;/a&gt; ) is that as long as you can frame what you care about as some sort of constrained optimization problem, the method of lagrange multipliers gives you a simple but powerful method to derive algorithms as well as understand and bound your problem‚Äôs sensitivites. I‚Äôve always found this to be really cool, because at it‚Äôs core you simply specify something you‚Äôd like to optimize (even if it‚Äôs non-convex), specify contraints that thing should ideally satisfy and like magic, out pops an algorithm or a function corresponding to the optimum (or if not you usually at least get computable conditions that optimum should satisfy).&lt;/p&gt;

&lt;p&gt;I‚Äôll walk through 3 examples of this type of constrained optimization that have popped up for me over the past month in the context of machine learning. Namely, using the Lagrange formalism as one perspective on how to  arrive at the origins of the Gaussian distribution, the Backprop algorithm, and the Trust Region Policy Optimization (TRPO) algorithm in reinforcement learning (in some ways the predecessor of the more widely used PPO algorithm).&lt;/p&gt;

&lt;h2 id=&quot;origins-of-the-gaussian-maximum-entropy-distributions&quot;&gt;Origins of the Gaussian (Maximum Entropy Distributions)&lt;/h2&gt;
&lt;p&gt;A natural question to ask is: What‚Äôs the most general distribution one can use to describe random variables with finite mean and finite variance?&lt;/p&gt;

&lt;p&gt;The entire point of statistics is to infer both the shape and the parameters that control the underlying distributions generating our samples. What‚Äôs the best we can do? Considering the space of all possible distributions we could potentially choose from we‚Äôd like to find a distribution that satisfies these constraints as weakly as possible. Having as little pre-existing structure as possible ensures it remains as general as possible, making the ‚Äúleast claim of being informed beyond the stated prior data‚Äù. What we‚Äôre really saying  is that we want a distribution that maximizes entropy&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;\(\text{maximize}[H(x)] = \text{maximize}[\int_{-\infty}^{\infty}p(x)\log(P(x)dx ]\).&lt;/p&gt;

&lt;p&gt;To reiterate the distribution must also satisfy the finite mean and finite variance constraints, explicitly:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Constraint 0&lt;/strong&gt; :Probability density function must some to 1:  \(\int_{-\infty}^{\infty}p(x)dx = 1\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Constraint 1&lt;/strong&gt; :Finite mean:  \(\int_{-\infty}^{\infty}p(x)xdx = \mu\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Constraint 2&lt;/strong&gt; :Finite variance: \(\int_{-\infty}^{\infty}p(x)\left(x-\mu\right)^2dx = \sigma\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Putting in all together using the method of Lagrange multipliers we have:&lt;/p&gt;

\[\begin{align}
\mathcal{L}= 
\int_{-\infty}^{\infty}p(x)\log p(x)dx 
-\lambda_0\left(\int_{-\infty}^{\infty}(p(x)-1\right)dx 
-\lambda_1\left(\int_{-\infty}^{\infty}p(x)(x)dx-\mu\right) \\
-\lambda_2\left(\int_{-\infty}^{\infty}p(x)(x-\mu)^2dx-\sigma\right) 
\end{align}\]

&lt;p&gt;Since \(\mathcal{L} = \mathcal{L}(P(x), \lambda)\), taking the gradients we have :&lt;/p&gt;

\[\begin{align}
\frac{\partial \mathcal{L}}{\partial p(x)} =  \log p(x) +1 - \lambda_0 - \lambda_1x -\lambda_2(x-\mu)^2 = 0 \\
\frac{\partial \mathcal{L}}{\partial \lambda_0}  = \int_{-\infty}^{\infty}p(x)dx-1 = 0\\\
\frac{\partial \mathcal{L}}{\partial \lambda_1} = \int_{-\infty}^{\infty}p(x)xdx-\mu = 0 \\
\frac{\partial \mathcal{L}}{\partial \lambda_2}  = \int_{-\infty}^{\infty}p(x)\left(x-\mu\right)^2dx-\sigma = 0 \\
\end{align}\]

&lt;p&gt;I won‚Äôt put the algebra here (it‚Äôs not at all interesting and you probably shouldn‚Äôt much care) but you start with&lt;/p&gt;

\[\log p(x) = \lambda_0 + \lambda_1 x + \lambda_2(x-\mu)^2 -1\]

&lt;p&gt;keep cranking and you‚Äôll eventually end up with the following:&lt;/p&gt;

\[P(x) = \frac{1}{\sqrt{2\pi \sigma^2}} exp\left(-\frac{(x-mu)^2}{2 \sigma^2}\right)\]

&lt;p&gt;The Gaussian distribution at last. To recap, by framing our original question as a constrained optimization problem we were able to arrive at the gaussian distribution in a pretty natural way.&lt;/p&gt;

&lt;h2 id=&quot;origins-of-backprop&quot;&gt;Origins of Backprop&lt;/h2&gt;
&lt;p&gt;It turns out we can reframe back-propagation in a &lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/lecun-88.pdf&quot;&gt;similar fashion&lt;/a&gt;. Ultimately, supervised learning is about minimizing the error between some labeled data \(Y\) and our model‚Äôs predictions, usually given by the last layer of our neural network \(Z^L\).&lt;/p&gt;

&lt;p&gt;Compactly: minimize \(Loss(z^L, y)\), where \(z^{i} = f^i(z^{i-1},W^i)\)&lt;/p&gt;

&lt;p&gt;At it‚Äôs core, it‚Äôs just another instance of a constrained optimization problem that can be tackled with the same Lagrangian formalism as before.&lt;/p&gt;

\[\mathcal{L}(z, W, \lambda)=  Loss(z^L, y) -\sum\limits_{i=1}^L \lambda_{i}^T\left(z^i - f^i(z^{i-1})\right)\]

&lt;p&gt;Above, we express the dynamics of each layer of the network as a sum through all \(L\) layers. Effectively, all we‚Äôre saying is ‚Äìminimize some loss function  &lt;em&gt;S‚Äôil vous pla√Æt&lt;/em&gt; ‚Äìwith the constraint that each layer \(Z^i\) of our network is just a function of the previous layer \(Z^{i-1}\) through some non-linear function \(f\).&lt;/p&gt;

&lt;p&gt;Taking the gradient we arrive at the following:&lt;/p&gt;

\[\begin{align}
\nabla_\boldsymbol{\lambda_i} \mathcal{L} = z^i - f^i(z^{i-1},W^i) = 0 \\
\nabla_\boldsymbol{w} \mathcal{L} = -\sum\limits_{i=1}^L \lambda_i^T \nabla_W f^i(z^{i-1},W^i) = 0 \\ 
\nabla_\boldsymbol{z^i} \mathcal{L} = \lambda_i - \nabla f^{i+1}(z^i,w^{i+1}) = 0 \\
\nabla_\boldsymbol{z^L} \mathcal{L} = \lambda_L - \nabla Loss(z^L, y) = 0 \\
\end{align}\]

&lt;p&gt;Taking a looking at above, one gets almost for free the following: 
\(z^i =f^i(z^{i-1}\) and \(\lambda_L = \nabla_{z^L} Loss(z^L,y)\)&lt;/p&gt;

&lt;p&gt;Additionally, Working through these (particularly the third equation)  one eventually arrives at&lt;/p&gt;

\[\lambda_j = \sum_{i \in \beta(j)} \lambda_i \frac{\partial f_i (z^i, W^i)}{\partial z^j}\]

&lt;p&gt;where \(\beta(j)\) is the set of all incoming edges from the vertex j&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Which is recognizable in the context of backprop as the equation for the the adjoints \(\lambda_ i\), telling us how to measure the sensitivity of one node relative to the previous layer.&lt;/p&gt;
&lt;h2 id=&quot;origins-of-trpo&quot;&gt;Origins of TRPO&lt;/h2&gt;
&lt;p&gt;Man, bad news homie ‚Äì this post is already much longer than I originally intended (and I‚Äôm getting &lt;em&gt;real&lt;/em&gt; tired to typing out LaTex), so maybe I won‚Äôt go into as much detail about this one. Anyway, It turns out one care arrive at the core of &lt;a href=&quot;https://arxiv.org/pdf/1502.05477.pdf&quot;&gt;TRPO&lt;/a&gt; algorithm in reinforcement learning under a similar scheme. I‚Äôll try to lay crux out briefly:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html&quot;&gt;Reinforcement learning&lt;/a&gt; fundamentally differs from supervised learning in some important ways. Particularly, in the supervised learning case your samples are drawn i.i.d from some underlying distribution and therefore it ultimately doesn‚Äôt really matter if you sample from a ‚Äúbad‚Äù (low reward) region of your parameter space in one batch, because your next batch will not be conditioned on the poor performing batch in any way. The issue in reinforcement learning&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; is that this sampling i.i.d assumption no longer holds, precisely because your policy ultimately also &lt;em&gt;controls&lt;/em&gt; your sampling process; your policy both learns from your samples and is responsible for gathering new samples.&lt;/p&gt;

&lt;p&gt;Ultimately we‚Äôd only like to trust our policy within some small region. Ostensibly, a natural way to do this is to bound our parameter updates such that they stay within some small radius of their original values.&lt;/p&gt;

\[\theta' \leftarrow  \arg\max_{\theta} (\theta' - \theta)^T \nabla J(\theta)\]

\[s.t \quad ||\theta' - \theta || ^2 \leq \epsilon\]

&lt;p&gt;In reality, however we don‚Äôt have much reason to expect this to be meaningful since some parameters might change much more quickly or slowly than others. Ultimately what we care about isn‚Äôt &lt;em&gt;really&lt;/em&gt; that the parameters that control out policy be bounded from one update to the next, it‚Äôs that change in the policy &lt;em&gt;itself&lt;/em&gt; be bounded. One way to do this is instead of restricting the gradients, we  should restrict the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;&gt;KL Divergence&lt;/a&gt; between the policy and it‚Äôs update to be bound by some epsilon ball. Sure sounds like a constrained optimization problem don‚Äôt it? üòâ&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Worked out in more detail &lt;a href=&quot;https://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/&quot;&gt;here&lt;/a&gt; for the interested¬†&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;At least in the on-policy case¬†&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 22 Nov 2020 04:56:29 -0800</pubDate>
        <link>http://localhost:4000/openai/2020/11/22/reframe-reparametrize/</link>
        <guid isPermaLink="true">http://localhost:4000/openai/2020/11/22/reframe-reparametrize/</guid>
      </item>
    
      <item>
        <title>Chasing the White Rabbit</title>
        <description>&lt;p&gt;Election week 2020 has been one of the longest 56 years of my life. Trying to get anything substantial accomplished in this context has been a humbling reminder of the limits of my will.&lt;/p&gt;

&lt;p&gt;The backdrop of chaos and existential dread not withstanding, in the past two weeks I‚Äôve begun wrestling more seriously with many of the same questions I imagine all early stage researchers find themselves struggling with.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;How do I efficiently allocate my time? What are the right problems? What subset of those are amenable to tractable progress?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Of course, responses to the core of how to address these questions have been crafted by a multitude of perspectives from many talented writers and research scientists&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. &lt;em&gt;Intellectually&lt;/em&gt; I think I hear what they‚Äôre saying, however, I‚Äôm still not quite sure I yet &lt;em&gt;grok&lt;/em&gt; what they mean.&lt;/p&gt;

&lt;h3 id=&quot;rabbit-holes&quot;&gt;Rabbit Holes&lt;/h3&gt;
&lt;p&gt;For my cohort these questions take increasing saliency as our project selection phase approaches. 
In the midst of this, one of the particularly frustrating tactical aspects of the past two weeks has been being stuck in rabbit holes. Frustrating, because attempting to align some foundational concept with your intuition  seems both educationally productive yet in absolute terms unduly time consuming. It‚Äôs difficult to decipher what the correct middle ground should be. After-all, wrestling deeply with problems and learning to get yourself unstuck is a critical skill. I‚Äôve talked to a couple people about this and through these discussions two major themes recur.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt; &lt;em&gt;Use the resources of those around you:&lt;/em&gt; One thing I've come to quickly realize is how ineffectively I'm leveraging the incredible talent and resources in my proximity. I need to become better at asking more questions to more people more frequently. There is something to be gained by learning to trod through problems independently, however it makes no sense to not take full advantage of the mentorship and knowledge base that's so readily accessible&lt;/li&gt;

&lt;li&gt; &lt;em&gt;Keeping context in mind, learn to work with imperfect information&lt;/em&gt; . The tendency to dive into rabbit holes is one I think is born from a desire to have perfect domain information. While obviously such an impulse can be helpful and even motivating, one of perhaps even more utility is the ability to contextualize and bound your uncertainty around particular concepts in order to ensure you don't allow relatively minor detail to occlude your view of the bigger picture. 
 &lt;/li&gt;

&lt;/ol&gt;

&lt;h2 id=&quot;what-ive-been-thinking-about&quot;&gt;What I‚Äôve been thinking about&lt;/h2&gt;
&lt;p&gt;Originally I actually planned to write this blog post as an attempt to breathe coherence into my thoughts on some linked concepts relating different research directions i‚Äôve been thinking about. Briefly, i‚Äôve been thinking about meta learning and operations that bake in inductive bias and various cognitive priors onto the latent space of attention models. Particularly mechanisms that would allow us to have weak formal guarantees on the  outputs produced by these models.  However, I‚Äôm not quite sure my thoughts are yet as organized on that front as i‚Äôd like for them to be. I‚Äôd like to come back and write about that some other time.&lt;/p&gt;

&lt;p&gt;By the way this entire post was written by GPT3. Just playing. Catch you on the flip!&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Hamming‚Äôs &lt;a href=&quot;https://www.cs.virginia.edu/~robins/YouAndYourResearch.pdf&quot;&gt;‚ÄúYou and Your research‚Äù&lt;/a&gt; and  Schulman‚Äôs &lt;a href=&quot;http://joschu.net/blog/opinionated-guide-ml-research.html&quot;&gt;‚ÄúOpinionated Guide to ML Research‚Äù&lt;/a&gt; come to mind¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 05 Nov 2020 04:56:29 -0800</pubDate>
        <link>http://localhost:4000/openai/2020/11/05/ay-dios-mio/</link>
        <guid isPermaLink="true">http://localhost:4000/openai/2020/11/05/ay-dios-mio/</guid>
      </item>
    
      <item>
        <title>OpenAIs Scholars Initial Thoughts</title>
        <description>&lt;p&gt;I‚Äôm starting a new blog&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;  to mark the beginning of my time as part of the OpenAI scholar‚Äôs program&lt;/p&gt;

&lt;h2 id=&quot;things-im-excited-about&quot;&gt;Things I‚Äôm excited about&lt;/h2&gt;
&lt;p&gt;I‚Äôm deeply excited and humbled to work with such an incredible and deeply talented pool of individuals, both within my cohort and within OpenAI in general. The first week has been such a whirlwind of excitement. I‚Äôve spent much of it shoring up my foundations in machine learning theory and application as well tyring to find my bearings in the wide sea of research interests.&lt;/p&gt;

&lt;p&gt;I joined this program because of my belief that well aligned machine intelligence can act as an incredible force multiplier for a wide range of human endeavors. I‚Äôm most excited by the incredible generality of deep learning; the remarkable ability to abstract and  apply the same set of algorithms to a wide range of seemingly disparate problems. This paradigm excites me because it  reimagines computers and computation as being more than numerical automata but as robust tools capable of ingesting and producing rich inputs and outputs allowing us to augment our own problem solving capacity and by extension amplify ingenuity and improve the human condition. Honestly, what could be more exciting or compelling as a research interest?&lt;/p&gt;

&lt;h2 id=&quot;ramblings&quot;&gt;Ramblings&lt;/h2&gt;
&lt;p&gt;One thing I‚Äôve personally found useful is keep running track of ideas that emerge during the course of self study. I‚Äôve found this idea tracking helpful in the meta-sense; in that, over time it helps me see emerging trends in my train of thought. I‚Äôm really not sure how useful these stream of consciousness type expositions will be for others, but nevertheless here are some of my thoughts from this week:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Training a neural network produces a model that we can think of as some object whose weights and biases provide discrete estimates of the high dimensional manifold our training data lies on. Along this train of thought, can we  think of the hyperparameters of two different networks trained on the same data as two different samples of the same underlying ‚Äòtrue‚Äô manifold? What then is the relationship (if any) between the weights of these two models? Is it possible to be clever and somehow combine these two manifold samplings to get a better estimate of the true manifold ala something loosely analogous to &lt;a href=&quot;https://francisbach.com/richardson-extrapolation/&quot;&gt;Richardson Extrapolation&lt;/a&gt; /Bayesian updating ?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When training neural networks we typically include as part of the loss function, a regularization term. The idea behind this is that penalizing large model weights we can prevent the co-adaptation of network weights and limit the networks ability to learn local noise in the underlying training data thus leading to better generalization.  The regularization terms i‚Äôve seen thus far are fairy straightforward l1 or l2 norms on the network weights. I‚Äôm curious to explore the effectiveness of regularizing with more sophisticated techniques. Like regularizing with the goal to minimize entropy or something along that vein.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;That‚Äôs it for now. Catch you all later!&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For the literally two people who read the single post in the old blog, I‚Äôve migrated that post as well :)¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 22 Oct 2020 05:56:29 -0700</pubDate>
        <link>http://localhost:4000/openai/pixylls/jeykll/2020/10/22/new-blog-who-dis/</link>
        <guid isPermaLink="true">http://localhost:4000/openai/pixylls/jeykll/2020/10/22/new-blog-who-dis/</guid>
      </item>
    
      <item>
        <title>Non-inertial Reference Frames and Coin Wishing Wells (Part I)</title>
        <description>&lt;p&gt;Sometime ago, a friend of mine and I were curious about the physical principles that cause coins to sprial so elegantly down the kinds of coin funnels one might find at the  local mall.
After searching online we were unable to find a description that was both mathematically satisfying and sufficiently captured the relevant physics. Having some free time, we decided to tackle this problem on our own&lt;/p&gt;

&lt;h3 id=&quot;lets-talk-lagrangians&quot;&gt;Let‚Äôs talk Lagrangians&lt;/h3&gt;

&lt;p&gt;Loosely speaking, in physics , whenever one is working on a trajectory problem that
involves non-cartesian coordinates it‚Äôs often easier to find the equations of motion
using the Lagrangian framework, which (as we‚Äôll see for this problem) is far more convenient for solving
problems that aren‚Äôt so well suited for Newton‚Äôs formulation of classical mechanics.&lt;/p&gt;

&lt;p&gt;In particular, the utility of  Lagrange‚Äôs framework is that it allows for us to work
in the so called &lt;em&gt;generalized coordinates&lt;/em&gt;. The principle being that if we‚Äôre being clever,
these generalized coordinates can (and indeed should) be chosen to exploit geometry or symmetries
present in the system we wish to analyze.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/OldBlog/Wishing_Well.jpeg&quot; alt=&quot;wishingWell&quot; width=&quot;400px&quot; /&gt;.&lt;/p&gt;

&lt;p&gt;Looking at this image of a coin well cross section, we can imagine that it might be
useful to describe the motion of a coin about the funnel in terms of it‚Äôs radius \(\hat{r}\) from the center  of the funnel,
it‚Äôs azimuthal angle \(\hat{\theta}\), and it‚Äôs height \(\hat{z}\) within the well.&lt;/p&gt;

&lt;p&gt;Above, the Lagrangian is a function of kinetic and potential energy. For the kinetic energy we have:&lt;/p&gt;

\[T(x,y,z)= \frac{1}{2}m(\dot{x}^2+\dot{y}^2 +\dot{z}^2)  \leftrightarrow  T=\frac{1}{2}m(\dot{r}^2+r^2\dot{\theta}^2+\dot{z}^2)\]

&lt;p&gt;the second expression for kinetic energy \(T\) follows since we have 
\(x = rcos(\theta)\) and \(y= rsin(\theta)\).
Then, since the height \(z\) is a function of the radius \(r\) we have&lt;/p&gt;

\[\dot{z}(r,\dot{r}) = \frac{c_1\dot{r}}{r^2} \leadsto T=\frac{1}{2}m(\dot{r}^2+r^2\dot{\theta}^2+\frac{c_1^2\dot{r}^2}{r^4})\]

&lt;p&gt;The potential energy for this system comes simply as a result of gravity. From kindergarten we have,&lt;/p&gt;

\[U(z) = mgz \rightarrow  U(r) = mg(-\frac{c_1}{r}+c_2)\]

&lt;p&gt;Therefore, we may write the lagrangian function \(\mathcal{L}=T-U\) as&lt;/p&gt;

\[\mathcal{L}=\frac{1}{2}m(\dot{r}^2+r^2\dot{\theta}^2+\frac{c_1^2\dot{r}^2}{r^4}) - g(\frac{c_1}{r}+c_2)\]

&lt;p&gt;At this point, before proceeding with Lagrange‚Äôs equation, it‚Äôs important to note that expression above doesn‚Äôt
account for non-conservative forces (like friction) that dissipate the total energy of the coin as it proceeds down the well.
It could be useful to model these viscous effects using the seemingly reasonable assumption that these frictional
forces scaled proportionally with radial and angular velocity It turns out afterward, that this is actually a damn fine first order approximation&lt;/p&gt;

&lt;p&gt;As an educated guess one could assume that this dissapative force might look something like&lt;/p&gt;

\[F = \frac{1}{2}(k_r\dot{r}^2+k_{\theta}\dot{\theta}^2)\]

&lt;p&gt;Where \(k_r\) and \(k_{\theta}\) are just some dissipative constants. This is the so called
&lt;a href=&quot;https://en.wikipedia.org/wiki/Rayleigh_dissipation_function&quot;&gt;Rayleigh dissipation function&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For the radial component ,the Lagrange Equation (taking into account the dissipative friction effects we might expect the coin well to follow) may be written as&lt;/p&gt;

\[\frac{\partial \mathcal{L}}{\partial r}-\frac{\partial \mathcal{L}}{\partial t}\frac{\partial \mathcal{L}}{\partial \dot{r}} = \frac{\partial F}{\partial \dot{r}}\]

&lt;p&gt;After substituting the lagrangian  function $\mathcal{L}$ into the expression above, and after some tedious algebra we finally arrive at our first equation of motion&lt;/p&gt;

\[\boxed{\ddot{r}(\frac{r^4+c_1^2}{r^4})-\frac{2c_1^2\dot{r}^2}{r^5}+\frac{gc_1}{r^2}-r\dot{\theta}^2=\frac{k_r\dot{r}}{m}}\]

&lt;p&gt;Bitchin‚Äô!&lt;/p&gt;

&lt;p&gt;Doing precisely the same thing for the \(\theta\) component, we arrive at the second equation of motion&lt;/p&gt;

\[\boxed{-(r^2\ddot{\theta}+2r\dot{r}\dot{\theta})=k\dot{\theta}}\]

&lt;p&gt;If you‚Äôre unfamiliar with Lagrangian mechanics, allow me to take a moment to explain why this result is significant:
The complete equations of motion for this system were obtained &lt;em&gt;despite&lt;/em&gt; the fact that we &lt;em&gt;only&lt;/em&gt; specified the kinetic and potential energies of the coin without any
reference to Newtonian theory i.e we were able to do this without ever explicitly considering the &lt;em&gt;forces&lt;/em&gt; acting on the coin.&lt;/p&gt;

&lt;h3 id=&quot;experiments-and-simulations&quot;&gt;Experiments and Simulations&lt;/h3&gt;
&lt;p&gt;I took the equations of motion above and plugged the differential equations above into MATLAB in order to simulate the coin‚Äôs trajectory.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/OldBlog/test_animate.gif&quot; alt=&quot;simulation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Taking just a cursory qualitative look at the general form of the trajectory looks remarkably like the trajectories one sees in the actual coin wells.
In the next part of this article, we‚Äôll take a look at how we can get more quantitative confidence that what we‚Äôve derived corresponds to reality. Stay tuned!&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Mon, 18 Nov 2019 04:56:29 -0800</pubDate>
        <link>http://localhost:4000/openai/pixylls/jeykll/2019/11/18/coin-wishing-well/</link>
        <guid isPermaLink="true">http://localhost:4000/openai/pixylls/jeykll/2019/11/18/coin-wishing-well/</guid>
      </item>
    
  </channel>
</rss>
